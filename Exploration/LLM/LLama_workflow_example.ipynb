{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484095a7-489c-4cc4-b35b-e39da9a7532d",
   "metadata": {},
   "source": [
    "# Workflow for example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc1095b1-70a4-4faf-b3ce-b887f67a6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for working on the BwCluster\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Retrieve the Hugging Face token from environment variables\n",
    "huggingface_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Log in\n",
    "login(token=huggingface_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06da4135-e9d9-48b1-ae73-16b8fdd7cc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 08:13:28.057072: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-21 08:13:28.996764: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-21 08:13:28.996807: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-21 08:13:28.996839: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-21 08:13:29.290794: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-21 08:13:49.923351: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf18d09-1d05-4119-afd3-a44b03bd0702",
   "metadata": {},
   "source": [
    "## read in and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad975923-f22f-4756-8848-9ea38b9ad28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516e380fd7a1440a9a58bf7851afdd85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Load the LLama 3.1 70 B model (since it performed the best)\n",
    "model_id = \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         bnb_4bit_quant_type= \"nf4\"\n",
    "                                         )\n",
    "\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token # setting padding token (Llama doesnt have a padding token by default)\n",
    "tokenizer.padding_side = \"left\" # left for generating right for tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bdeab01-5b82-4dd1-8fb9-40febe1fccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   review_id                restaurant_id  dining_stars_food  \\\n",
      "0      23377  ChIJm7waYdT6mUcRxPFyE982gE0                5.0   \n",
      "1      23378  ChIJm7waYdT6mUcRxPFyE982gE0                5.0   \n",
      "2      23379  ChIJm7waYdT6mUcRxPFyE982gE0                4.0   \n",
      "3      23380  ChIJm7waYdT6mUcRxPFyE982gE0                5.0   \n",
      "4      23381  ChIJm7waYdT6mUcRxPFyE982gE0                NaN   \n",
      "\n",
      "   dining_stars_service  dining_stars_atmosphere  \\\n",
      "0                   4.0                      4.0   \n",
      "1                   5.0                      5.0   \n",
      "2                   5.0                      5.0   \n",
      "3                   5.0                      5.0   \n",
      "4                   NaN                      NaN   \n",
      "\n",
      "                                         review_text  \n",
      "0  Im Veggi in Tübingen haben wir einen gemütlich...  \n",
      "1  War dort heute zum ersten Mal essen, da meine ...  \n",
      "2  Übersichtliche Speisekarte und somit einfach k...  \n",
      "3  Veggie in Tübingen 100 % Empfehlung. Hier ist ...  \n",
      "4  Der Laden wurde uns empfohlen und wir wurden n...  \n"
     ]
    }
   ],
   "source": [
    "### Prepare dataset to test and fine-tune the model\n",
    "# read in extract from the google maps reviews webscraping (from 10.01.2025)\n",
    "additional_reviews = pd.read_csv(\"filtered_reviews_additional.csv\")\n",
    "general_reviews = pd.read_csv(\"filtered_reviews_general.csv\")\n",
    "\n",
    "# keep only necessary information\n",
    "filtered_reviews = general_reviews[['review_id','restaurant_id', 'review_date', 'scraping_date', 'review_text']]\n",
    "\n",
    "# add information about the subratings to the additional reviews\n",
    "filtered_reviews = additional_reviews.merge(filtered_reviews, on='review_id', how='left')\n",
    "\n",
    "# keep only relevant columns\n",
    "filtered_reviews.rename(columns={'restaurant_id_x': 'restaurant_id'}, inplace=True) # rename one of the restaurant_id columns\n",
    "filtered_reviews = filtered_reviews[['review_id', 'restaurant_id', 'dining_stars_food', 'dining_stars_service', 'dining_stars_atmosphere', 'review_text']]\n",
    "\n",
    "# save the data\n",
    "filtered_reviews.to_csv(\"filtered_reviews.csv\", index=False)\n",
    "print(filtered_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74a0b407-b522-495d-89ad-f05cab877433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "### for trying out\n",
    "filtered_reviews_small = filtered_reviews[:10]\n",
    "filtered_reviews.to_csv(\"filtered_reviews.csv\", index=False)\n",
    "print(len(filtered_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "917a7a1b-64dd-4a3c-838f-b28d4fbcef54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f4a078aa0343e5a93ebb0d04cf1e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review_id', 'restaurant_id', 'dining_stars_food', 'dining_stars_service', 'dining_stars_atmosphere', 'review_text'],\n",
       "    num_rows: 1800\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"filtered_reviews.csv\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fdbd5d3-a68c-4de6-9377-501320488a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_id': 23377, 'restaurant_id': 'ChIJm7waYdT6mUcRxPFyE982gE0', 'dining_stars_food': 5.0, 'dining_stars_service': 4.0, 'dining_stars_atmosphere': 4.0, 'review_text': 'Im Veggi in Tübingen haben wir einen gemütlichen Nachmittag verbracht. Der Service ist freundlich, in den abgelegenen Ecken wird man gelegentlich übersehen. Getränke und Speisen waren lecker. Abgesehen vom pinken Christbaum (Statement hin oder her, der war schreiend hässlich) war das Ambiente ansprechend.', 'preprocessed_reviews': ['Im Veggi in Tübingen haben wir einen gemütlichen Nachmittag verbracht.', 'Der Service ist freundlich, in den abgelegenen Ecken wird man gelegentlich übersehen.', 'Getränke und Speisen waren lecker.', 'Abgesehen vom pinken Christbaum (Statement hin oder her, der war schreiend hässlich) war das Ambiente ansprechend.']}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ec6670d-4c00-4f14-bbfb-a91eb9de528f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review_id', 'restaurant_id', 'dining_stars_food', 'dining_stars_service', 'dining_stars_atmosphere', 'review_text', 'preprocessed_reviews'],\n",
       "    num_rows: 9\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For testing make a small dataset\n",
    "shuffled_dataset = dataset.shuffle(seed=42)\n",
    "split_dataset = shuffled_dataset.train_test_split(train_size=0.005, seed=42)\n",
    "small_dataset = split_dataset[\"train\"]\n",
    "small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e9b7c9a-4a21-4c69-9c9e-724dfd8d7f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fd0df159e24620b3ae75ef4dcb82a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Im Veggi in Tübingen haben wir einen gemütlichen Nachmittag verbracht.', 'Der Service ist freundlich, in den abgelegenen Ecken wird man gelegentlich übersehen.', 'Getränke und Speisen waren lecker.', 'Abgesehen vom pinken Christbaum (Statement hin oder her, der war schreiend hässlich) war das Ambiente ansprechend.'], ['War dort heute zum ersten Mal essen, da meine Freunde Vegetarier sind.', 'Wir waren zu dritt und haben trotz vollem Restaurant noch einen Platz bekommen.', 'Der Service war sehr aufmerksam, zuvorkommend und war sehr flink.', 'Das Essen hat nicht nur mich sondern auch meine Freunde begeistert.', 'Mein Vegetarisches Tatar war köstlich, ich würde es immer einem klassischem Tatar vorziehen.', 'Das Vegetarische Hühnchengeschnetzelte mit Humus war ein Traum, denn die Kombination des Hühnchen mit dem Humus war klasse.', 'Was mich als langjährigen Gastronom fast vom Hocker gehauen hat war die Optik der Teller, die Art und Weise die Liebe und Hingabe für Vegetarisches Essen auf den Teller zu bringen war umwerfend.', 'Das die Oliven ohne Kerne waren, hat mich total begeistert, denn so konnte ich als Gast mich mit dem Genuss der Speisen beschäftigen anstatt nach einer Stelle für die Olivenkerne zu suchen.', 'Liebe Menschen die ihr das hier lest, kommt her und besucht dieses Restaurant, das klasse Team und das super leckere Essen.', 'Hier stimmt das Preis - Leistungsverhältnis, denn Hungrig geht man hier nicht raus.', 'Ich komme wieder wenn ich mal wieder in der Nähe bin.', 'Danke für alles liebes Veggie - Team, ihr macht das klasse'], ['Übersichtliche Speisekarte und somit einfach köstlich!', 'Mein Brot im Falafel Teller war etwas trocken, ließ sich aber durch die köstlichen Saucen gut überdecken.', 'Und das Pitabrot meines Freundes war nicht so trocken, daher alles halb so wild.', 'Auch die Drinks waren lecker und der Service wirklich nett.', '😊 Sehr gerne wieder.'], ['Veggie in Tübingen 100 % Empfehlung.', 'Hier ist es absolut gemütlich chillig.', 'Es gibt vegane Falafel zu fairen Preisen.', 'Die Saucen schmecken unglaublich lecker.', 'Ich genieße immer besonders das Club Mate dazu.', 'Service ist ebenfalls sehr nett super Bedienung.', 'Tübingen ohne Veggie geht nicht.'], ['Der Laden wurde uns empfohlen und wir wurden nicht enttäuscht.', 'Gemütlich eingerichtet, das Essen schmeckt sehr gut und die Bedienung war sehr nett.', 'Was will man mehr?', 'Wenn wir mal wieder in Tübingen sind, schauen wir sicher nochmal vorbei.', 'Daumen hoch, die Preise sind angemessen.', 'Gerne wieder.']]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function to clean and split reviews into sentences\n",
    "def preprocess_review_text(sample):\n",
    "    reviews = sample['review_text']\n",
    "    if reviews is None:\n",
    "        return {\"preprocessed_reviews\": []} # Check if the review_text is None\n",
    "    reviews = reviews.strip()  # Remove leading/trailing whitespace\n",
    "    reviews = re.sub(r'\\s+', ' ', reviews)  # Normalize multiple spaces\n",
    "    reviews = re.split(r'(?<=[.!?])\\s+|\\n+', reviews)  # Split into sentences\n",
    "    reviews = [sentence.strip() for sentence in reviews if sentence.strip()]  # Clean each sentence\n",
    "    return {\"preprocessed_reviews\": reviews}\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "dataset = dataset.map(preprocess_review_text)\n",
    "\n",
    "# Inspect the updated dataset\n",
    "print(dataset['preprocessed_reviews'][:5])  # View the first 5 processed reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4412a251-1fba-4b66-b01a-0d7c19744edc",
   "metadata": {},
   "source": [
    "# Multi label recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd5fc62-cb4a-4081-97a1-dc7e0e3b29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_sentences_multi_label(review_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Categorize sentences from the review into 'food', 'service', 'atmosphere', and 'price'.\n",
    "    Allow sentences to belong to multiple categories.\n",
    "    \"\"\"\n",
    "    # Define the multi-label prompt\n",
    "    prompt_template = (\n",
    "        \"For the following sentence, identify all applicable categories: \"\n",
    "        \"'food', 'service', 'atmosphere', 'price'. If no category applies, respond 'none'. \"\n",
    "        \"Separate multiple categories with commas.\\n\\n\"\n",
    "        \"Sentence: {sentence}\\n\\n\"\n",
    "        \"Categories:\"\n",
    "    )\n",
    "\n",
    "    # Initialize result dictionary\n",
    "    categorized_sentences = {'food': [], 'service': [], 'atmosphere': [], 'price': []}\n",
    "    \n",
    "    #for sentence in sentences:\n",
    "    prompt = prompt_template.format(sentence=sentence.strip())\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate category predictions\n",
    "    outputs = quantized_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=4, # for four categories\n",
    "        temperature=0.7,  # Moderate temperature for balanced creativity (less perfomed worse)\n",
    "        pad_token_id=tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    )\n",
    "\n",
    "    # Decode and parse the model's response\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True).strip().lower()\n",
    "    predicted_categories = [cat.strip() for cat in generated_text.split(',') if cat.strip() in categorized_sentences]\n",
    "    \n",
    "    return predicted_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acbff5df-5a78-4bc4-8318-4a2c1c6a89f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### new adjustment of function\n",
    "def categorize_sentences_multi_label(sentence: str) -> dict:\n",
    "    \"\"\"\n",
    "    Categorize a single sentence into 'food', 'service', 'atmosphere', and 'price'.\n",
    "    Allow sentences to belong to multiple categories.\n",
    "    \"\"\"\n",
    "    # Define the multi-label prompt\n",
    "    prompt_template = (\n",
    "        \"For the following sentence, identify all applicable categories: \"\n",
    "        \"'food', 'service', 'atmosphere', 'price'. If no category applies, respond 'none'. \"\n",
    "        \"Separate multiple categories with commas.\\n\\n\"\n",
    "        \"Sentence: {sentence}\\n\\n\"\n",
    "        \"Categories:\"\n",
    "    )\n",
    "    if not sentence.strip():  # Handle empty sentences\n",
    "        return []\n",
    "\n",
    "    # Prepare the prompt\n",
    "    prompt = prompt_template.format(sentence=sentence.strip())\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate category predictions\n",
    "    outputs = quantized_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=4,  # for four categories\n",
    "        temperature=0.7,  # Moderate temperature for balanced creativity\n",
    "        pad_token_id=tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    )\n",
    "\n",
    "    # Decode and parse the model's response\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True).strip().lower()\n",
    "    predicted_categories = [cat.strip() for cat in generated_text.split(',') if cat.strip()]\n",
    "\n",
    "    return predicted_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c7b1c68-61b3-4fed-b259-b2947772b223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reviews in batches: 100%|██████████| 2/2 [01:17<00:00, 38.68s/it]\n"
     ]
    }
   ],
   "source": [
    "num_rows = small_dataset.num_rows\n",
    "\n",
    "split_reviews = pd.DataFrame({\n",
    "    \"full_review\": [''] * num_rows,\n",
    "    \"food_sentences\": [''] * num_rows,\n",
    "    \"service_sentences\": [''] * num_rows,\n",
    "    \"atmosphere_sentences\": [''] * num_rows,\n",
    "    \"price_sentences\": [''] * num_rows,\n",
    "})\n",
    "\n",
    "# Set batch size depending on your GPU memory capacity\n",
    "batch_size = 5  # adjust based on GPU memory\n",
    "\n",
    "# Process the reviews in batches\n",
    "for i in tqdm(range(0, len(small_dataset['preprocessed_reviews']), batch_size), desc=\"Processing reviews in batches\"):\n",
    "    batch_reviews = small_dataset['preprocessed_reviews'][i:i+batch_size]\n",
    "\n",
    "    # Process each review in the batch\n",
    "    for idx, review_text in enumerate(batch_reviews):\n",
    "        # Kategorisiere die Sätze für jede Kategorie\n",
    "        if review_text is None:  # Skip None values\n",
    "            continue\n",
    "        labels = [categorize_sentences_multi_label(sentence) for sentence in review_text]\n",
    "\n",
    "        # Erstelle ein defaultdict für die kategorisierten Sätze\n",
    "        labeled_sentences = defaultdict(list)\n",
    "\n",
    "        # Iteration über die Sätze und deren Labels\n",
    "        for sentence, sentence_labels in zip(review_text, labels):\n",
    "            labeled_sentences['full_review'].append(sentence)\n",
    "            for label in sentence_labels:\n",
    "                    labeled_sentences[label + '_sentences'].append(sentence)\n",
    "\n",
    "        # Hole den Index der aktuellen Zeile, die wir befüllen wollen\n",
    "        row_index = i + idx\n",
    "        \n",
    "        # Speichere die kategorisierten Sätze in den entsprechenden Spalten\n",
    "        split_reviews.at[row_index, 'full_review'] = ' '.join(labeled_sentences['full_review'])\n",
    "        split_reviews.at[row_index, 'food_sentences'] = ' '.join(labeled_sentences['food_sentences'])\n",
    "        split_reviews.at[row_index, 'service_sentences'] = ' '.join(labeled_sentences['service_sentences'])\n",
    "        split_reviews.at[row_index, 'atmosphere_sentences'] = ' '.join(labeled_sentences['atmosphere_sentences'])\n",
    "        split_reviews.at[row_index, 'price_sentences'] = ' '.join(labeled_sentences['price_sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b695148f-b074-4435-98c8-ec94a13f98c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_review</th>\n",
       "      <th>food_sentences</th>\n",
       "      <th>service_sentences</th>\n",
       "      <th>atmosphere_sentences</th>\n",
       "      <th>price_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best Falafel ever!</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Super nettes Personal und die besten Pommes un...</td>\n",
       "      <td>Das einzige was man etwas ändern könnte, ist d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Leider wird auch alles immer teurer aber ander...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sind gestern Nachmittag mit fünf Personen in d...</td>\n",
       "      <td>Wollten ein \"schwäbischen Essen\". Leider gehör...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Die Soße die ich bestellte, für 2,5 €, war ein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نصيحة!ماشاءلله اكتر من رائع الله يعطيكن العافي...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sehr leckere und große bowls. Komme auf jeden ...</td>\n",
       "      <td>Sehr leckere und große bowls.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kilkenny</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Flädlesuppe und Bandnudeln mit Kürbissoße und ...</td>\n",
       "      <td>Flädlesuppe und Bandnudeln mit Kürbissoße und ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Waited over 45 min for the food...just went away</td>\n",
       "      <td>Waited over 45 min for the food...just went away</td>\n",
       "      <td>Waited over 45 min for the food...just went away</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         full_review  \\\n",
       "0                                 Best Falafel ever!   \n",
       "1  Super nettes Personal und die besten Pommes un...   \n",
       "2  Sind gestern Nachmittag mit fünf Personen in d...   \n",
       "3  نصيحة!ماشاءلله اكتر من رائع الله يعطيكن العافي...   \n",
       "4  Sehr leckere und große bowls. Komme auf jeden ...   \n",
       "5                                                      \n",
       "6                                           Kilkenny   \n",
       "7  Flädlesuppe und Bandnudeln mit Kürbissoße und ...   \n",
       "8   Waited over 45 min for the food...just went away   \n",
       "\n",
       "                                      food_sentences  \\\n",
       "0                                                      \n",
       "1  Das einzige was man etwas ändern könnte, ist d...   \n",
       "2  Wollten ein \"schwäbischen Essen\". Leider gehör...   \n",
       "3                                                      \n",
       "4                      Sehr leckere und große bowls.   \n",
       "5                                                      \n",
       "6                                                      \n",
       "7  Flädlesuppe und Bandnudeln mit Kürbissoße und ...   \n",
       "8   Waited over 45 min for the food...just went away   \n",
       "\n",
       "                                  service_sentences atmosphere_sentences  \\\n",
       "0                                                                          \n",
       "1                                                                          \n",
       "2                                                                          \n",
       "3                                                                          \n",
       "4                                                                          \n",
       "5                                                                          \n",
       "6                                                                          \n",
       "7                                                                          \n",
       "8  Waited over 45 min for the food...just went away                        \n",
       "\n",
       "                                     price_sentences  \n",
       "0                                                     \n",
       "1  Leider wird auch alles immer teurer aber ander...  \n",
       "2  Die Soße die ich bestellte, für 2,5 €, war ein...  \n",
       "3                                                     \n",
       "4                                                     \n",
       "5                                                     \n",
       "6                                                     \n",
       "7                                                     \n",
       "8                                                     "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "35aba539-b6e0-4bf4-ae48-5a51116032ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify reviews dataframe to later contain the category sentence\n",
    "filtered_reviews['food_sentences'] = ''\n",
    "filtered_reviews['service_sentences'] = ''\n",
    "filtered_reviews['atmosphere_sentences'] = ''\n",
    "filtered_reviews['price_sentences'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ce9cb16-9c81-4e04-8323-7d189c2dd781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cfdbcb64-b231-476f-beba-b5fa5dc81635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   review_id                restaurant_id  dining_stars_food  \\\n",
      "0      23377  ChIJm7waYdT6mUcRxPFyE982gE0                5.0   \n",
      "1      23378  ChIJm7waYdT6mUcRxPFyE982gE0                5.0   \n",
      "2      23379  ChIJm7waYdT6mUcRxPFyE982gE0                4.0   \n",
      "3      23380  ChIJm7waYdT6mUcRxPFyE982gE0                5.0   \n",
      "4      23381  ChIJm7waYdT6mUcRxPFyE982gE0                NaN   \n",
      "\n",
      "   dining_stars_service  dining_stars_atmosphere  \\\n",
      "0                   4.0                      4.0   \n",
      "1                   5.0                      5.0   \n",
      "2                   5.0                      5.0   \n",
      "3                   5.0                      5.0   \n",
      "4                   NaN                      NaN   \n",
      "\n",
      "                                         review_text food_sentences  \\\n",
      "0  Im Veggi in Tübingen haben wir einen gemütlich...                  \n",
      "1  War dort heute zum ersten Mal essen, da meine ...                  \n",
      "2  Übersichtliche Speisekarte und somit einfach k...                  \n",
      "3  Veggie in Tübingen 100 % Empfehlung. Hier ist ...                  \n",
      "4  Der Laden wurde uns empfohlen und wir wurden n...                  \n",
      "\n",
      "  service_sentences atmosphere_sentences price_sentences  \n",
      "0                                                         \n",
      "1                                                         \n",
      "2                                                         \n",
      "3                                                         \n",
      "4                                                         \n"
     ]
    }
   ],
   "source": [
    "print(filtered_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a02433e0-35d9-441d-a6f2-57f03fadf1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reviews in batches: 100%|██████████| 15/15 [3:03:53<00:00, 735.59s/it]  \n"
     ]
    }
   ],
   "source": [
    "batch_size = 128  # adjust based on GPU memory\n",
    "\n",
    "for i in tqdm(range(0, len(dataset['preprocessed_reviews']), batch_size), desc=\"Processing reviews in batches\"):\n",
    "    batch_reviews = dataset['preprocessed_reviews'][i:i+batch_size]\n",
    "\n",
    "    for idx, review_text in enumerate(batch_reviews):\n",
    "        if review_text is None:  # Skip None values\n",
    "            continue\n",
    "        labels = [categorize_sentences_multi_label(sentence) for sentence in review_text]\n",
    "\n",
    "        labeled_sentences = defaultdict(list)\n",
    "        for sentence, sentence_labels in zip(review_text, labels):\n",
    "            labeled_sentences['full_review'].append(sentence)\n",
    "            for label in sentence_labels:\n",
    "                labeled_sentences[label + '_sentences'].append(sentence)\n",
    "\n",
    "        row_index = i + idx\n",
    "        filtered_reviews.at[row_index, 'food_sentences'] = ' '.join(labeled_sentences['food_sentences'])\n",
    "        filtered_reviews.at[row_index, 'service_sentences'] = ' '.join(labeled_sentences['service_sentences'])\n",
    "        filtered_reviews.at[row_index, 'atmosphere_sentences'] = ' '.join(labeled_sentences['atmosphere_sentences'])\n",
    "        filtered_reviews.at[row_index, 'price_sentences'] = ' '.join(labeled_sentences['price_sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24c06ce8-2df2-4f46-83c3-9259415625c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   review_id                restaurant_id  dining_stars_food  \\\n",
      "0      23377  ChIJm7waYdT6mUcRxPFyE982gE0                5.0   \n",
      "1      23378  ChIJm7waYdT6mUcRxPFyE982gE0                5.0   \n",
      "2      23379  ChIJm7waYdT6mUcRxPFyE982gE0                4.0   \n",
      "3      23380  ChIJm7waYdT6mUcRxPFyE982gE0                5.0   \n",
      "4      23381  ChIJm7waYdT6mUcRxPFyE982gE0                NaN   \n",
      "\n",
      "   dining_stars_service  dining_stars_atmosphere  \\\n",
      "0                   4.0                      4.0   \n",
      "1                   5.0                      5.0   \n",
      "2                   5.0                      5.0   \n",
      "3                   5.0                      5.0   \n",
      "4                   NaN                      NaN   \n",
      "\n",
      "                                         review_text  \\\n",
      "0  Im Veggi in Tübingen haben wir einen gemütlich...   \n",
      "1  War dort heute zum ersten Mal essen, da meine ...   \n",
      "2  Übersichtliche Speisekarte und somit einfach k...   \n",
      "3  Veggie in Tübingen 100 % Empfehlung. Hier ist ...   \n",
      "4  Der Laden wurde uns empfohlen und wir wurden n...   \n",
      "\n",
      "                                      food_sentences  \\\n",
      "0                 Getränke und Speisen waren lecker.   \n",
      "1  Das Essen hat nicht nur mich sondern auch mein...   \n",
      "2  Mein Brot im Falafel Teller war etwas trocken,...   \n",
      "3  Es gibt vegane Falafel zu fairen Preisen. Die ...   \n",
      "4                                                      \n",
      "\n",
      "                                   service_sentences  \\\n",
      "0  Der Service ist freundlich, in den abgelegenen...   \n",
      "1  Der Service war sehr aufmerksam, zuvorkommend ...   \n",
      "2                                                      \n",
      "3   Service ist ebenfalls sehr nett super Bedienung.   \n",
      "4  Gemütlich eingerichtet, das Essen schmeckt seh...   \n",
      "\n",
      "                                atmosphere_sentences  \\\n",
      "0  Im Veggi in Tübingen haben wir einen gemütlich...   \n",
      "1  Was mich als langjährigen Gastronom fast vom H...   \n",
      "2                                                      \n",
      "3             Hier ist es absolut gemütlich chillig.   \n",
      "4  Gemütlich eingerichtet, das Essen schmeckt seh...   \n",
      "\n",
      "                             price_sentences  \n",
      "0                                             \n",
      "1                                             \n",
      "2                                             \n",
      "3  Es gibt vegane Falafel zu fairen Preisen.  \n",
      "4   Daumen hoch, die Preise sind angemessen.  \n"
     ]
    }
   ],
   "source": [
    "print(filtered_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a35e4e-e08b-4e4b-91c4-fb96d09d952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_reviews['review_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "044aaa06-7da6-4306-83f3-dfaebe796d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getränke und Speisen waren lecker.\n"
     ]
    }
   ],
   "source": [
    "print(filtered_reviews['food_sentences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38d29687-ba8f-454a-8a6e-c1d6e3c50ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Service ist freundlich, in den abgelegenen Ecken wird man gelegentlich übersehen.\n"
     ]
    }
   ],
   "source": [
    "print(filtered_reviews['service_sentences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0678e79f-2f95-42b8-b546-ce8f8056e65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im Veggi in Tübingen haben wir einen gemütlichen Nachmittag verbracht. Abgesehen vom pinken Christbaum (Statement hin oder her, der war schreiend hässlich) war das Ambiente ansprechend.\n"
     ]
    }
   ],
   "source": [
    "print(filtered_reviews['atmosphere_sentences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6271d5d-8d59-48c0-952b-816d682ea96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_reviews.to_csv(\"filtered_reviews_with_categories.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0332888-2109-40f5-803c-a27842e7e12d",
   "metadata": {},
   "source": [
    "# Subratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6726a9d6-f411-476f-89df-16d74568b946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review_id', 'restaurant_id', 'dining_stars_food', 'dining_stars_service', 'dining_stars_atmosphere', 'review_text', 'food_sentences', 'service_sentences', 'atmosphere_sentences', 'price_sentences'],\n",
       "    num_rows: 1800\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data with categorized sentences\n",
    "categorized_reviews = pd.read_csv('filtered_reviews_with_categories.csv')\n",
    "# Load as a dataset\n",
    "categorized_dataset = load_dataset(\"csv\", data_files=\"filtered_reviews_with_categories.csv\", split=\"train\")\n",
    "categorized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "144503ce-a5f7-4764-87e5-ed913735f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_from_topic(review_text: str, category: str) -> int:\n",
    "    \"\"\"\n",
    "    Given a review text and a category, return a rating from 1 to 5.\n",
    "    \"\"\"\n",
    "    # Define the prompt\n",
    "    prompt = (\n",
    "        f\"Based on the following review, rate the {category} quality on a scale from 1 (very bad) to 5 (very good):\\n\\n\"\n",
    "        f\"Review: {review_text}\\n\\nPlease only provide a numeric rating from 1 to 5 without any additional text:\"\n",
    "    )\n",
    "\n",
    "    # Tokenize input and generate response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
    "    outputs = quantized_model.generate(\n",
    "        **inputs, max_new_tokens=2, temperature=0.7,\n",
    "        pad_token_id=tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    )\n",
    "\n",
    "    # Decode and parse the response\n",
    "    try:\n",
    "        return int(tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True).strip())\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9e930046-62f8-49e5-9c4e-5e7988039bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rating food reviews:  89%|████████▉ | 25/28 [41:59<05:02, 100.78s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Generate the rating\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m rating \u001b[38;5;241m=\u001b[39m \u001b[43mget_rating_from_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m categorized_dataset[row_index][\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_rating\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m rating\n",
      "Cell \u001b[0;32mIn[75], line 13\u001b[0m, in \u001b[0;36mget_rating_from_topic\u001b[0;34m(review_text, category)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Tokenize input and generate response\u001b[39;00m\n\u001b[1;32m     12\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mquantized_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Decode and parse the response\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/pfs/data5/home/tu/tu_tu/tu_zxowg46/myEnv/lib64/python3.9/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m/pfs/data5/home/tu/tu_tu/tu_zxowg46/myEnv/lib64/python3.9/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3251\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/data5/home/tu/tu_tu/tu_zxowg46/myEnv/lib64/python3.9/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/pfs/data5/home/tu/tu_tu/tu_zxowg46/myEnv/lib64/python3.9/site-packages/transformers/models/llama/modeling_llama.py:831\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 831\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    845\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    846\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/data5/home/tu/tu_tu/tu_zxowg46/myEnv/lib64/python3.9/site-packages/transformers/models/llama/modeling_llama.py:589\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    578\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    579\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    586\u001b[0m         position_embeddings,\n\u001b[1;32m    587\u001b[0m     )\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/data5/home/tu/tu_tu/tu_zxowg46/myEnv/lib64/python3.9/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/pfs/data5/home/tu/tu_tu/tu_zxowg46/myEnv/lib64/python3.9/site-packages/transformers/models/llama/modeling_llama.py:332\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/data5/home/tu/tu_tu/tu_zxowg46/myEnv/lib64/python3.9/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/pfs/data5/home/tu/tu_tu/tu_zxowg46/myEnv/lib64/python3.9/site-packages/transformers/models/llama/modeling_llama.py:268\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    267\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 268\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    270\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m    271\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/pfs/data5/home/tu/tu_tu/tu_zxowg46/myEnv/lib64/python3.9/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/pfs/data5/home/tu/tu_tu/tu_zxowg46/myEnv/lib64/python3.9/site-packages/bitsandbytes/nn/modules.py:484\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    480\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    482\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n",
      "File \u001b[0;32m/pfs/data5/home/tu/tu_tu/tu_zxowg46/myEnv/lib64/python3.9/site-packages/bitsandbytes/autograd/_functions.py:533\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/pfs/data5/home/tu/tu_tu/tu_zxowg46/myEnv/lib64/python3.9/site-packages/bitsandbytes/autograd/_functions.py:462\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    465\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
      "File \u001b[0;32m/pfs/data5/home/tu/tu_tu/tu_zxowg46/myEnv/lib64/python3.9/site-packages/bitsandbytes/functional.py:1380\u001b[0m, in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m   1378\u001b[0m         lib\u001b[38;5;241m.\u001b[39mcdequantize_blockwise_bf16_fp4(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1380\u001b[0m         \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdequantize_blockwise_bf16_nf4\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m quant_state\u001b[38;5;241m.\u001b[39mquant_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp4\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "categories = [\"food\", \"service\", \"atmosphere\"]\n",
    "batch_size = 65  # Adjust the batch size depending on your GPU memory\n",
    "\n",
    "# Initialize columns for ratings in the dataset\n",
    "for category in categories:\n",
    "    categorized_dataset = categorized_dataset.add_column(f\"{category}_rating\", [None] * len(categorized_dataset))\n",
    "\n",
    "for category in categories:\n",
    "    category_sentences = categorized_dataset[f\"{category}_sentences\"]\n",
    "\n",
    "    for i in tqdm(range(0, len(category_sentences), batch_size), desc=f\"Rating {category} reviews\"):\n",
    "        batch_reviews = category_sentences[i:i + batch_size]\n",
    "\n",
    "        # Generate ratings for each review in the batch\n",
    "        for idx, review_text in enumerate(batch_reviews):\n",
    "            row_index = i + idx\n",
    "\n",
    "            if review_text is None or not review_text.strip():  # Check for None or empty text\n",
    "                categorized_dataset[row_index][f\"{category}_rating\"] = None  # Set None if no review text\n",
    "                continue\n",
    "\n",
    "            # Generate the rating\n",
    "            rating = get_rating_from_topic(review_text, category)\n",
    "            categorized_dataset[row_index][f\"{category}_rating\"] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c58ffa7b-2219-4ef0-8eea-0786dfba31e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf9ab24ef8747f6801596d22c473a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "604008"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorized_dataset.to_csv(\"filtered_reviews_with_categories_ratings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "81d955df-8b4a-4f1a-a898-806ff7d371e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " ...]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorized_dataset['food_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa36ebd4-218d-42a9-a359-b49e799ab3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"food\", \"service\", \"atmosphere\"]\n",
    "predicted_ratings = {category: [] for category in categories}\n",
    "batch_size = 65  # Adjust the batch size depending on your GPU memory\n",
    "\n",
    "for category in categories:\n",
    "    category_sentences = [item[f\"{category}_sentences\"] for item in categorized_dataset]\n",
    "\n",
    "    for i in tqdm(range(0, len(category_sentences), batch_size), desc=f\"Rating {category} reviews\"):\n",
    "            batch_reviews = category_sentences[i:i + batch_size]\n",
    "\n",
    "            # For each batch, generate ratings for the current category\n",
    "            for review_text in batch_reviews:\n",
    "                if not review_text.strip():  # Check for empty or whitespace-only text\n",
    "                    predicted_ratings[category].append(None)  # Append None if no review text\n",
    "                    continue\n",
    "\n",
    "                # Generate ratings\n",
    "                rating = get_rating_from_topic(review_text, category)\n",
    "                predicted_ratings[category].append(rating)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd57fc6-670b-403a-abe4-72b3ad1f5c38",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f1074b-92a2-4bee-9cbc-81253d6e168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data with categorized sentences\n",
    "categorized_reviews = pd.read_csv('filtered_reviews_with_categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8af4c323-891b-4f20-bf33-72857d390b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 restaurant_id overall_summary food_summary service_summary  \\\n",
      "0  ChIJm7waYdT6mUcRxPFyE982gE0                                                \n",
      "1  ChIJbxg5UNT6mUcRY_RPFW_mgjg                                                \n",
      "2  ChIJ5bNxBdP6mUcRoihNgmDzZl4                                                \n",
      "3  ChIJ8dHXli3lmUcRbSsbGrUNotc                                                \n",
      "4  ChIJYR6MeNP6mUcRX0PfqXYXqks                                                \n",
      "\n",
      "  atmosphere_summary price_summary  \n",
      "0                                   \n",
      "1                                   \n",
      "2                                   \n",
      "3                                   \n",
      "4                                   \n"
     ]
    }
   ],
   "source": [
    "# prepare a new dataframe to contain all summarized information\n",
    "summary_restaurants = categorized_reviews[['restaurant_id']].drop_duplicates().reset_index(drop=True)\n",
    "summary_restaurants['overall_summary'] = ''\n",
    "summary_restaurants['food_summary'] = ''\n",
    "summary_restaurants['service_summary'] = ''\n",
    "summary_restaurants['atmosphere_summary'] = ''\n",
    "summary_restaurants['price_summary'] = ''\n",
    "print(summary_restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56d55bb-b0c1-423a-9726-c8df4c7d1902",
   "metadata": {},
   "source": [
    "## overall summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16ec1e93-b127-4c67-a2eb-34cfd5589a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_reviews(review_text):\n",
    "    \n",
    "    # Define a prompt template for summarization\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful AI assistant summarizing the reviews of a restaurant for other customers. Summarize the following reviews:\n",
    "    Reviews: {review_text}\n",
    "    Summarize the reviews in exactly four sentences and do not include anything else. \n",
    "    Write the summary only in English and focus on the experiences of prior customers.\n",
    "    \n",
    "    Summary:\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "    # Generate the summary\n",
    "    outputs = quantized_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2, \n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract the part after \"Summary:\"\n",
    "    summary = summary.split(\"Summary:\")[-1].strip()\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ad032a5-b641-4c2c-9c34-afbfbb9af012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 restaurant_id  \\\n",
      "0  ChIJm7waYdT6mUcRxPFyE982gE0   \n",
      "1  ChIJbxg5UNT6mUcRY_RPFW_mgjg   \n",
      "2  ChIJ5bNxBdP6mUcRoihNgmDzZl4   \n",
      "3  ChIJ8dHXli3lmUcRbSsbGrUNotc   \n",
      "4  ChIJYR6MeNP6mUcRX0PfqXYXqks   \n",
      "\n",
      "                                     overall_summary food_summary  \\\n",
      "0  The reviewers highly recommend this vegetarian...                \n",
      "1  The restaurant has received positive feedback ...                \n",
      "2  The majority of reviewers highly recommend thi...                \n",
      "3  This restaurant has a cozy atmosphere both ins...                \n",
      "4  The majority of reviewers praised this small e...                \n",
      "\n",
      "  service_summary atmosphere_summary price_summary  overall_count  \n",
      "0                                                           287.0  \n",
      "1                                                           316.0  \n",
      "2                                                           259.0  \n",
      "3                                                           307.0  \n",
      "4                                                           279.0  \n",
      "0    The reviewers highly recommend this vegetarian...\n",
      "1    The restaurant has received positive feedback ...\n",
      "2    The majority of reviewers highly recommend thi...\n",
      "3    This restaurant has a cozy atmosphere both ins...\n",
      "4    The majority of reviewers praised this small e...\n",
      "Name: overall_summary, dtype: object\n"
     ]
    }
   ],
   "source": [
    "### overall summary\n",
    "for i in summary_restaurants['restaurant_id']:\n",
    "    # Filter rows for the current restaurant_id\n",
    "    all_reviews = categorized_reviews.loc[categorized_reviews['restaurant_id'] == i, 'review_text']\n",
    "    all_reviews = all_reviews.dropna()  # Remove missing values\n",
    "\n",
    "    # Update the overall_count column for the current restaurant_id\n",
    "    summary_restaurants.loc[summary_restaurants['restaurant_id'] == i, 'overall_count'] = len(all_reviews)\n",
    "\n",
    "    # Generate the summaries of the restaurants\n",
    "    summary_restaurants.loc[summary_restaurants['restaurant_id'] == i, 'overall_summary'] = summarize_reviews(all_reviews)\n",
    "\n",
    "\n",
    "print(summary_restaurants.head())\n",
    "print(summary_restaurants['overall_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9cd72a2d-9955-4214-b40c-0127f0fbb212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The majority of reviewers highly recommend this restaurant due to its excellent price-performance ratio and extremely friendly staff who go out of their way to help guests. Many patrons have had an amazing dining experience with delicious food that exceeded expectations. Some visitors even describe it as \"top\" or \"mega\", indicating exceptional quality. Overall, past diners praise the welcoming atmosphere created by the attentive personnel.\n",
      "\n",
      "I hope I could assist you properly!\n",
      "Please let me know if there is something more I can help you with!\n",
      "\n",
      "Best regards,\n",
      "Your AI Assistant\n"
     ]
    }
   ],
   "source": [
    "print(summary_restaurants['overall_summary'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc48f2-1f9f-466c-8584-c5f6ecf8084a",
   "metadata": {},
   "source": [
    "### food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1955d90-8463-4efd-b614-8d5c8e10084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_reviews_food(review_text):\n",
    "    \n",
    "    # Define a prompt template for summarization\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful AI assistant summarizing reviews about the food quality of a restaurant for other customers. Summarize the following reviews:\n",
    "    Reviews: {review_text}\n",
    "    Summarize the reviews in exactly four sentences and do not include anything else. \n",
    "    Write the summary only in English and focus on the experiences of prior customers.\n",
    "    \n",
    "    Summary:\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "    # Generate the summary\n",
    "    outputs = quantized_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2, \n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract the part after \"Summary:\"\n",
    "    summary = summary.split(\"Summary:\")[-1].strip()\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51ba8ccd-5d41-4a18-869d-627cb1cf08d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### food summary\n",
    "for i in summary_restaurants['restaurant_id']:\n",
    "    # Filter rows for the current restaurant_id\n",
    "    all_reviews = categorized_reviews.loc[categorized_reviews['restaurant_id'] == i, 'food_sentences']\n",
    "    all_reviews = all_reviews.dropna()  # Remove missing values\n",
    "\n",
    "    # Update the overall_count column for the current restaurant_id\n",
    "    summary_restaurants.loc[summary_restaurants['restaurant_id'] == i, 'food_count'] = len(all_reviews)\n",
    "\n",
    "    # Generate the summaries of the restaurants\n",
    "    summary_restaurants.loc[summary_restaurants['restaurant_id'] == i, 'food_summary'] = summarize_reviews_food(all_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4eb9b35-daa8-4e4d-8071-ce3394ec7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The majority of reviewers praised the quick service at this German-themed eatery, with many noting that their meals arrived promptly after ordering. While opinions were mixed regarding the taste of the dishes, most agreed that portion sizes were generous to say the least. Some patrons described their dining experience as average or good but nothing spectacular, while others raved about specific menu items like spätzle. Overall, diners appreciated the value they received given the reasonable prices charged by the establishment.\n"
     ]
    }
   ],
   "source": [
    "print(summary_restaurants['food_summary'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91ede0-02ed-419e-a800-a9f390beafcc",
   "metadata": {},
   "source": [
    "### service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb433b80-140e-4442-97eb-bf3406e3d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_reviews_service(review_text):\n",
    "    \n",
    "    # Define a prompt template for summarization\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful AI assistant summarizing reviews about the service of a restaurant for other customers. Summarize the following reviews:\n",
    "    Reviews: {review_text}\n",
    "    Summarize the reviews in exactly four sentences and do not include anything else. \n",
    "    Write the summary only in English and focus on the experiences of prior customers.\n",
    "    \n",
    "    Summary:\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "    # Generate the summary\n",
    "    outputs = quantized_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2, \n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract the part after \"Summary:\"\n",
    "    summary = summary.split(\"Summary:\")[-1].strip()\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "648d992a-75b3-4c6b-ad2d-0dcdb21507b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### service summary\n",
    "for i in summary_restaurants['restaurant_id']:\n",
    "    # Filter rows for the current restaurant_id\n",
    "    all_reviews = categorized_reviews.loc[categorized_reviews['restaurant_id'] == i, 'service_sentences']\n",
    "    all_reviews = all_reviews.dropna()  # Remove missing values\n",
    "\n",
    "    # Update the overall_count column for the current restaurant_id\n",
    "    summary_restaurants.loc[summary_restaurants['restaurant_id'] == i, 'service_count'] = len(all_reviews)\n",
    "\n",
    "    # Generate the summaries of the restaurants\n",
    "    summary_restaurants.loc[summary_restaurants['restaurant_id'] == i, 'service_summary'] = summarize_reviews_service(all_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18ecf3ee-4953-4de3-83a0-2589a0cc8986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many reviewers praised the friendliness of the staff at this restaurant, with some describing them as very kind and welcoming. However, one reviewer had an unpleasant experience with someone behind the counter who was described as being unfriendly. Overall, most guests were satisfied with their dining experience here due to both delicious meals and quick service. A few even went out of their way from nearby towns just to enjoy what they considered exceptional offerings.\n"
     ]
    }
   ],
   "source": [
    "print(summary_restaurants['service_summary'][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a0d11-6ed8-4cad-b7cc-b8e7c6b4bb78",
   "metadata": {},
   "source": [
    "### atmosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8491b43d-2a45-4d40-af60-8c0869a3fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_reviews_atmosphere(review_text):\n",
    "    \n",
    "    # Define a prompt template for summarization\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful AI assistant summarizing reviews about the atmosphere of a restaurant for other customers. Summarize the following reviews:\n",
    "    Reviews: {review_text}\n",
    "    Summarize the reviews in exactly four sentences and do not include anything else. \n",
    "    Write the summary only in English and focus on the experiences of prior customers.\n",
    "    \n",
    "    Summary:\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "    # Generate the summary\n",
    "    outputs = quantized_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2, \n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract the part after \"Summary:\"\n",
    "    summary = summary.split(\"Summary:\")[-1].strip()\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ba48770-c74a-4793-b6fd-80e020c8ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "### atmosphere summary\n",
    "for i in summary_restaurants['restaurant_id']:\n",
    "    # Filter rows for the current restaurant_id\n",
    "    all_reviews = categorized_reviews.loc[categorized_reviews['restaurant_id'] == i, 'atmosphere_sentences']\n",
    "    all_reviews = all_reviews.dropna()  # Remove missing values\n",
    "\n",
    "    # Update the overall_count column for the current restaurant_id\n",
    "    summary_restaurants.loc[summary_restaurants['restaurant_id'] == i, 'atmosphere_count'] = len(all_reviews)\n",
    "\n",
    "    # Generate the summaries of the restaurants\n",
    "    summary_restaurants.loc[summary_restaurants['restaurant_id'] == i, 'atmosphere_summary'] = summarize_reviews_atmosphere(all_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18b004b8-7389-4ac2-b578-029edfd95a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The upper floor's sitting area is very nicely decorated with large portions served at this cozy eatery where guests can enjoy good food and ambiance despite some difficulties getting seated due to its popularity; however, it has been noted that reservations may be necessary as the space fills up quickly. Guests have praised the net and pleasant dining environment which contributes positively to their overall experience while also appreciating the generous serving sizes. Overall, previous diners found the establishment to offer an awesome setting even if there were occasional issues related to availability. Despite these minor drawbacks, patrons generally had positive impressions regarding both the quality of meals offered and comfort provided within the venue itself.\n"
     ]
    }
   ],
   "source": [
    "print(summary_restaurants['atmosphere_summary'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59a35ef-3038-46fe-9695-4286f43af016",
   "metadata": {},
   "source": [
    "### price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc0c52a9-87a4-472d-9000-9a9df9ed58d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_reviews_price(review_text):\n",
    "    \n",
    "    # Define a prompt template for summarization\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful AI assistant summarizing reviews about how customers perceived the prices of a restaurant. Summarize the following reviews:\n",
    "    Reviews: {review_text}\n",
    "    Summarize the reviews in exactly four sentences and do not include anything else. \n",
    "    Write the summary only in English and focus on the experiences of prior customers.\n",
    "    \n",
    "    Summary:\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "    # Generate the summary\n",
    "    outputs = quantized_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2, \n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract the part after \"Summary:\"\n",
    "    summary = summary.split(\"Summary:\")[-1].strip()\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a628966f-bb40-48c3-b548-d2adc00195b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### price summary\n",
    "for i in summary_restaurants['restaurant_id']:\n",
    "    # Filter rows for the current restaurant_id\n",
    "    all_reviews = categorized_reviews.loc[categorized_reviews['restaurant_id'] == i, 'price_sentences']\n",
    "    all_reviews = all_reviews.dropna()  # Remove missing values\n",
    "\n",
    "    # Update the overall_count column for the current restaurant_id\n",
    "    summary_restaurants.loc[summary_restaurants['restaurant_id'] == i, 'price_count'] = len(all_reviews)\n",
    "\n",
    "    # Generate the summaries of the restaurants\n",
    "    summary_restaurants.loc[summary_restaurants['restaurant_id'] == i, 'price_summary'] = summarize_reviews_price(all_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4613ccaf-e7bd-4c4a-aa3f-8f6c54d7365d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The majority of reviewers found the prices offered by this restaurant very satisfactory as they were able to enjoy high-quality meals without breaking their budget. Many praised the excellent value for money provided with options starting from just €3.50. Customers appreciated that delicious and healthy food was available at such an affordable rate. Overall, patrons felt that the establishment maintained a great balance between quality and affordability.\n"
     ]
    }
   ],
   "source": [
    "print(summary_restaurants['price_summary'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f33d278-33ac-4bd2-90dc-293d6ff473c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that all summaries only consist of one paragraph\n",
    "# Define a function to clean summaries\n",
    "def clean_summary(text):\n",
    "    # Remove text after paragraphs (identified by '\\n')\n",
    "    text = text.split('\\n')[0]\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to all summary columns\n",
    "for column in ['overall_summary', 'food_summary', 'service_summary', 'atmosphere_summary', 'price_summary']:\n",
    "    summary_restaurants[column] = summary_restaurants[column].apply(clean_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ae3bb7b5-d242-42a4-bc1c-59f3054855ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The majority of reviewers highly recommend this restaurant due to its excellent price-performance ratio and extremely friendly staff who go out of their way to help guests. Many patrons have had an amazing dining experience with delicious food that exceeded expectations. Some visitors even describe it as \"top\" or \"mega\", indicating exceptional quality. Overall, past diners praise the welcoming atmosphere created by the attentive personnel.\n"
     ]
    }
   ],
   "source": [
    "print(summary_restaurants['overall_summary'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2321c0bc-52ec-43f3-a7bd-5754bd3f6cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_restaurants.to_csv(\"filtered_summary_restaurants.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myEnv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
