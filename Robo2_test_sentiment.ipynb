{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import time\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=\"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\" # Tier 2 key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split tasks into a maximum of two batches\n",
    "def split_into_batches(tasks, batch_size=45000):\n",
    "    if len(tasks) <= batch_size:\n",
    "        return [tasks]\n",
    "    return [tasks[:batch_size], tasks[batch_size:batch_size*2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data from reviews_general table\n",
    "data = pd.read_csv(\"reviews_general_selected.csv\") # as an example\n",
    "\n",
    "# preprocess the data\n",
    "data = data.dropna(subset=['review_text']) # drop rows with missing review_text\n",
    "data['review_text'] = data['review_text'].str.replace(r'\\s+', ' ', regex=True).str.strip() # remove extra spaces, newlines, and tabs\n",
    "data = data[data[\"review_text\"].str.len() > 10] # drop rows where the \"review_text\" column contains less than 10 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for testing purposes, we will only use the first 1000 reviews\n",
    "data = data.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for topic extraction\n",
    "topic_extraction_prompt = '''\n",
    "You are an expert at structured data extraction. Extract sentences from restaurant reviews that mention the following topics: food, service, atmosphere, and price.\n",
    "Return the results in the following JSON format:\n",
    "\n",
    "{\n",
    "    \"food_sentences\": string[], \n",
    "    \"service_sentences\": string[], \n",
    "    \"atmosphere_sentences\": string[], \n",
    "    \"price_sentences\": string[]\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create tasks for batch processing\n",
    "def create_batch_tasks_topic_extraction(data):\n",
    "    tasks = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        review_text = row['review_text']\n",
    "        review_id = row['review_id']  # Access the review_id from the DataFrame\n",
    "        \n",
    "        task = {\n",
    "            \"custom_id\": f\"{review_id}\",  # Use review_id in the custom_id\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "                \"temperature\": 0.1,\n",
    "                \"response_format\": {\n",
    "                    \"type\": \"json_object\"\n",
    "                },\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": topic_extraction_prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": review_text\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "        tasks.append(task)\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "# Prepare the batch tasks as a list\n",
    "tasks_topic_extraction = create_batch_tasks_topic_extraction(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split tasks into batches of 4500 each\n",
    "batches_topic_extraction = split_into_batches(tasks_topic_extraction, batch_size=45000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save each batch to a separate .jsonl file\n",
    "def save_batches_to_jsonl(batches, prefix):\n",
    "    batch_file_names = []\n",
    "    \n",
    "    for i, batch in enumerate(batches):\n",
    "        batch_file_name = f\"{prefix}_part{i+1}.jsonl\"\n",
    "        with open(batch_file_name, 'w') as file:\n",
    "            for task in batch:\n",
    "                file.write(json.dumps(task) + '\\n')\n",
    "        batch_file_names.append(batch_file_name)\n",
    "    \n",
    "    return batch_file_names  # Return the list of saved file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload batch files to the API\n",
    "def upload_batch_files(batch_file_names):\n",
    "    batch_file_ids = []\n",
    "    \n",
    "    for batch_file_name in batch_file_names:\n",
    "        batch_file = client.files.create(\n",
    "            file=open(batch_file_name, \"rb\"),\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "        print(f\"Batch file uploaded: {batch_file.id}\")\n",
    "        batch_file_ids.append(batch_file.id)\n",
    "    \n",
    "    return batch_file_ids  # Return the list of uploaded file IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to start batch jobs\n",
    "def start_batch_jobs(batch_file_ids):\n",
    "    batch_job_ids = []\n",
    "    \n",
    "    for file_id in batch_file_ids:\n",
    "        batch_job = client.batches.create(\n",
    "            input_file_id=file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "        )\n",
    "        print(f\"Batch job started: {batch_job.id}\")\n",
    "        batch_job_ids.append(batch_job.id)\n",
    "    \n",
    "    return batch_job_ids  # Return the list of batch job IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_file_names_topic_extraction = save_batches_to_jsonl(batches_topic_extraction, prefix=\"batch_tasks_topic_extraction\")\n",
    "batch_file_ids_topic_extraction = upload_batch_files(batch_file_names_topic_extraction)\n",
    "batch_job_ids_topic_extraction = start_batch_jobs(batch_file_ids_topic_extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Overall summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts for the overall summaries\n",
    "OVERALL_SUMMARY_PROMPT = (\n",
    "    \"You are an expert summarizer specializing in restaurant reviews. Summarize the following reviews for a restaurant. \"\n",
    "    \"Be sure to include the overall tone of the reviews. \"\n",
    "    \"Write concisely, strictly in English and limit the overall response to around 400 characters.\\n\\n\"\n",
    "    \"Reviews:\\n{reviews}\\n\\n\"\n",
    "    \"Summary:\"\n",
    ")\n",
    "\n",
    "OVERALL_SUMMARY_COMBINE_PROMPT = (\n",
    "    \"You are an expert summarizer specializing in restaurant reviews. \"\n",
    "    \"Combine the following two summaries into a single concise and cohesive summary in English. \"\n",
    "    \"The summary should be limited to around 200 characters:\\n\\n\"\n",
    "    \"Summary 1:\\n{summary1}\\n\\n\"\n",
    "    \"Summary 2:\\n{summary2}\\n\\n\"\n",
    "    \"Combined Summary:\"\n",
    ")\n",
    "\n",
    "# Function to summarize a single chunk of reviews using modular prompts\n",
    "def summarize_chunk(prompt, reviews_chunk):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt.format(reviews=reviews_chunk)},\n",
    "            ],\n",
    "            max_tokens=200,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing chunk: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Function to combine summaries using modular prompts\n",
    "def combine_summaries(combine_prompt, summary1, summary2):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": combine_prompt.format(summary1=summary1, summary2=summary2)},\n",
    "            ],\n",
    "            max_tokens=200,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining summaries: {e}\")\n",
    "        return f\"{summary1}\\n\\n{summary2}\"\n",
    "\n",
    "    \n",
    "\n",
    "# Function to handle larger price reviews, retry, and combine summaries\n",
    "def summarize_reviews(restaurant_id, reviews_df, category_column_name, summary_prompt, combine_prompt):\n",
    "    \"\"\"\n",
    "    Generalized function to summarize reviews for a specific aspect (overall, service, atmosphere, etc.).\n",
    "    \n",
    "    Args:\n",
    "        restaurant_id (int): ID of the restaurant.\n",
    "        reviews_df (DataFrame): DataFrame containing review data.\n",
    "        category_column_name (str): Column in the DataFrame containing the reviews for this aspect.\n",
    "        summary_prompt (str): Prompt for summarizing reviews.\n",
    "        combine_prompt (str): Prompt for combining summaries.\n",
    "    \n",
    "    Returns:\n",
    "        str: Final summarized review.\n",
    "    \"\"\"\n",
    "    # Filter and join the reviews for the specified column\n",
    "    reviews = reviews_df[reviews_df['restaurant_id'] == restaurant_id][category_column_name].str.strip().replace([\"\", \"nan\", \"None\", \"null\"], pd.NA).dropna().astype(str).tolist()\n",
    "    \n",
    "    # If no reviews exist, return None\n",
    "    if not reviews:\n",
    "        return None, 0  # No summary, zero review count\n",
    "    \n",
    "    # Join the reviews into a single text chunk\n",
    "    reviews_text = \"\\n\".join(reviews)\n",
    "\n",
    "    # Getting amount of reviews\n",
    "    count_reviews = len(reviews)\n",
    "    \n",
    "    try:\n",
    "        # Attempt to summarize the full chunk\n",
    "        return summarize_chunk(summary_prompt, reviews_text), count_reviews\n",
    "    \n",
    "    except Exception as e:\n",
    "        if \"context_length_exceeded\" in str(e):\n",
    "            print(f\"Context length exceeded for restaurant '{restaurant_id}' in column '{category_column_name}'. Splitting reviews...\")\n",
    "            \n",
    "            # Split reviews into halves\n",
    "            mid_point = len(reviews) // 2\n",
    "            first_half = \"\\n\".join(reviews[:mid_point])\n",
    "            second_half = \"\\n\".join(reviews[mid_point:])\n",
    "            \n",
    "            # Summarize each half\n",
    "            first_summary = summarize_chunk(summary_prompt, first_half)\n",
    "            second_summary = summarize_chunk(summary_prompt, second_half)\n",
    "            \n",
    "            if first_summary and second_summary:\n",
    "                # Combine summaries\n",
    "                try:\n",
    "                    return combine_summaries(combine_prompt, first_summary, second_summary), count_reviews\n",
    "                except Exception as combine_error:\n",
    "                    print(f\"Error combining summaries for restaurant '{restaurant_id}' in column '{category_column_name}': {combine_error}\")\n",
    "                    return f\"{first_summary}\", (len(reviews) // 2)  # Return the first summary if combining fails\n",
    "            else:\n",
    "                return f\"{first_summary or 'Error in first half'}\\n\\n{second_summary or 'Error in second half'}\", (len(reviews) // 2)\n",
    "        else:\n",
    "            print(f\"Error summarizing reviews for restaurant '{restaurant_id}' in column '{category_column_name}': {e}\")\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 restaurant_id  \\\n",
      "0  ChIJ_VWb4xn6mUcRH4NujtHMKJI   \n",
      "1  ChIJo5EYOK_4mUcRi4shjNiEDUc   \n",
      "2  ChIJ_VfiMxj6mUcRRK_QBdxww7g   \n",
      "\n",
      "                                     overall_summary  user_count_overall  \n",
      "0  The reviews for this café and bakery are mixed...                  77  \n",
      "1  Der Boxenstop in Dusslingen is a vibrant bar p...                  21  \n",
      "2  The reviews express a highly positive tone tow...                   2  \n"
     ]
    }
   ],
   "source": [
    "# Extract unique restaurant IDs of the two batches\n",
    "unique_restaurant_ids = data['restaurant_id'].dropna().unique()\n",
    "\n",
    "summaries = []  # List to store summaries\n",
    "\n",
    "# Generate summaries for each restaurant\n",
    "for restaurant_id in unique_restaurant_ids:\n",
    "    # Summarize overall reviews\n",
    "    overall_summary, user_count_overall = summarize_reviews(\n",
    "        restaurant_id, data, 'review_text', \n",
    "        OVERALL_SUMMARY_PROMPT, OVERALL_SUMMARY_COMBINE_PROMPT\n",
    "    )\n",
    "    \n",
    "    # Append the summaries to the list\n",
    "    summaries.append({\n",
    "        \"restaurant_id\": restaurant_id,\n",
    "        \"overall_summary\": overall_summary,\n",
    "        \"user_count_overall\": user_count_overall\n",
    "    })\n",
    "\n",
    "# Convert the list of summaries into a DataFrame\n",
    "summaries_df = pd.DataFrame(summaries)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(summaries_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transition - wait until 1. finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check batch job status\n",
    "def wait_for_batches_to_complete(batch_job_ids):\n",
    "    while True:\n",
    "        all_done = True\n",
    "        for batch_job_id in batch_job_ids:\n",
    "            batch_job = client.batches.retrieve(batch_job_id)\n",
    "            status = batch_job.status\n",
    "            print(f\"Batch Job {batch_job_id} Status: {status}\")\n",
    "\n",
    "            if status != 'completed':\n",
    "                all_done = False  # Keep waiting if any batch is still running\n",
    "\n",
    "        if all_done:\n",
    "            print(\"All batch jobs are completed.\")\n",
    "            break  # Exit loop when all batches are done\n",
    "        \n",
    "        print(f\"Waiting 60 seconds before checking again...\")\n",
    "        time.sleep(60)  # Wait before checking again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for all batch jobs to complete\n",
    "wait_for_batches_to_complete(client, batch_job_ids_topic_extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Retrieve categorized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve batch results\n",
    "def retrieve_batch_results(batch_job_ids):\n",
    "    all_results = []\n",
    "\n",
    "    for batch_job_id in batch_job_ids:\n",
    "        # Get the output file ID\n",
    "        batch_job_status = client.batches.retrieve(batch_job_id)\n",
    "        result_file_id = batch_job_status.output_file_id\n",
    "\n",
    "        # ! think about error handeling !\n",
    "        #if not result_file_id:\n",
    "        #    print(f\" No output file found for batch job {batch_job_id}. Skipping...\")\n",
    "        #    continue\n",
    "\n",
    "        # Download the results file\n",
    "        result_content = client.files.content(result_file_id).content\n",
    "        result_file_name = f\"batch_results_{batch_job_id}.jsonl\"\n",
    "        with open(result_file_name, 'wb') as file:\n",
    "            file.write(result_content)  # Save the content to a file\n",
    "\n",
    "        # Parse the results\n",
    "        with open(result_file_name, 'r') as file:\n",
    "            for line in file:\n",
    "                all_results.append(json.loads(line.strip()))\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Function to extract sentences for each topic\n",
    "def extract_categorized_sentences(response):\n",
    "    try:\n",
    "        # Parse the assistant's message content\n",
    "        content = json.loads(response['body']['choices'][0]['message']['content'])\n",
    "        return {\n",
    "            \"food_sentences\": \" \".join(content.get(\"food_sentences\", [])),\n",
    "            \"service_sentences\": \" \".join(content.get(\"service_sentences\", [])),\n",
    "            \"atmosphere_sentences\": \" \".join(content.get(\"atmosphere_sentences\", [])),\n",
    "            \"price_sentences\": \" \".join(content.get(\"price_sentences\", [])),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "        return {\n",
    "            \"food_sentences\": None,\n",
    "            \"service_sentences\": None,\n",
    "            \"atmosphere_sentences\": None,\n",
    "            \"price_sentences\": None,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\AppData\\Local\\Temp\\ipykernel_22476\\3843819516.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  batches_to_keys = batches_to_keys.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "### for testing purposes\n",
    "batches_to_keys = pd.read_csv(\"batches_OpenAI.csv\")\n",
    "# strip all whitespace from the values\n",
    "batches_to_keys = batches_to_keys.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "# keep only examples with one api key\n",
    "batches_to_keys = batches_to_keys[batches_to_keys[\"api_key\"]==\"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\"]\n",
    "# keep only 15 examples\n",
    "batches_to_keys = batches_to_keys.head(15)\n",
    "client = OpenAI(\n",
    "  api_key=\"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\"\n",
    ")\n",
    "\n",
    "# resemble batch_job_ids_topic_extraction with batches_to_keys['batch_job_id']\n",
    "batch_job_ids_topic_extraction = batches_to_keys['batch_job_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing response: Unterminated string starting at: line 6 column 9 (char 215)\n",
      "Error parsing response: Expecting value: line 3 column 16413 (char 16438)\n",
      "Error parsing response: Unterminated string starting at: line 3 column 5 (char 88)\n",
      "Error parsing response: Unterminated string starting at: line 8 column 5 (char 288)\n",
      "                                      food_sentences  \\\n",
      "0  Aber das das was ich hier erleben durfte war d...   \n",
      "1  Der Kaffee hat gut geschmeckt und der Muffin w...   \n",
      "2  Die Qualität lässt sehr zu wünschen übrig sieh...   \n",
      "3                                                      \n",
      "4                                                      \n",
      "\n",
      "                                   service_sentences  \\\n",
      "0       Ich fühle mich echt vor den Kopf gestoßen!!!   \n",
      "1                                                      \n",
      "2                                                      \n",
      "3                                                      \n",
      "4  Dame mit kurzen weißen Haaren hat mich nicht b...   \n",
      "\n",
      "                                atmosphere_sentences  \\\n",
      "0                                                      \n",
      "1  Drinnen kann man sich hinsetzen und hat einen ...   \n",
      "2                                                      \n",
      "3  Aber wenn's von Außen schon so hübsch ist, wie...   \n",
      "4                                                      \n",
      "\n",
      "                                     price_sentences review_id  \n",
      "0  Ein Frühstück für 8,25€ ist ja okay das darf a...      4471  \n",
      "1                                                         4472  \n",
      "2                                                         4473  \n",
      "3                                                         4474  \n",
      "4                                                         4475  \n"
     ]
    }
   ],
   "source": [
    "# Retrieve and process results\n",
    "results_df = retrieve_batch_results(batch_job_ids_topic_extraction)\n",
    "\n",
    "# Apply the extraction to the 'response' column\n",
    "category_data = results_df['response'].apply(extract_categorized_sentences)\n",
    "\n",
    "# Create a new DataFrame with the extracted category sentences\n",
    "category_df = pd.DataFrame(category_data.tolist())\n",
    "\n",
    "# Combine 'custom_id' as 'review_id' and extracted category sentences\n",
    "category_df['review_id'] = results_df['custom_id'].astype('int64')\n",
    "\n",
    "\n",
    "# Display the first few rows\n",
    "print(category_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for sentiment analysis\n",
    "sentiment_prompt = (\n",
    "    \"Rate the sentiment of the following sentences on a scale of 1 to 5, \"\n",
    "    \"where 1 is 'very bad' and 5 is 'very good'. Return the result as a JSON object with the key 'rating'. \"\n",
    "    \"Only include the JSON object in your response.\\n\\n\"\n",
    "    \"Sentences: {sentences}\\n\\n\"\n",
    "    \"JSON:\"\n",
    ")\n",
    "\n",
    "# Define a function to create batch tasks for any category\n",
    "def create_sentiment_batch(data, category):\n",
    "    tasks = []\n",
    "    category_column = f\"{category}_sentences\"\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        sentences = row[category_column]\n",
    "        review_id = row['review_id']  # Access the review_id\n",
    "\n",
    "        if not sentences or pd.isna(sentences):  # Skip empty or NaN sentences\n",
    "            continue\n",
    "\n",
    "        task = {\n",
    "            \"custom_id\": f\"{review_id}_{category}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "                \"temperature\": 0.1,\n",
    "                \"response_format\": {\n",
    "                    \"type\": \"json_object\"\n",
    "                },\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a sentiment analysis expert specializing in restaurant reviews.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": sentiment_prompt.format(sentences=sentences)\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "        tasks.append(task)\n",
    "    \n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the batch tasks as a list\n",
    "food_tasks = create_sentiment_batch(category_df, \"food\")\n",
    "\n",
    "# Split tasks into batches of 45000 each\n",
    "batches_food = split_into_batches(food_tasks, batch_size=45000)\n",
    "\n",
    "batch_file_names_food = save_batches_to_jsonl(batches_food, prefix=\"batch_tasks_food\")\n",
    "batch_file_ids_food = upload_batch_files(batch_file_names_food)\n",
    "batch_job_ids_food = start_batch_jobs(batch_file_ids_food)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the batch tasks as a list\n",
    "service_tasks = create_sentiment_batch(category_df, \"service\")\n",
    "\n",
    "# Split tasks into batches of 45000 each\n",
    "batches_service = split_into_batches(service_tasks, batch_size=45000)\n",
    "\n",
    "batch_file_names_service = save_batches_to_jsonl(batches_service, prefix=\"batch_tasks_service\")\n",
    "batch_file_ids_service = upload_batch_files(batch_file_names_service)\n",
    "batch_job_ids_service = start_batch_jobs(batch_file_ids_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Atmosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the batch tasks as a list\n",
    "atmosphere_tasks = create_sentiment_batch(category_df, \"atmosphere\")\n",
    "\n",
    "# Split tasks into batches of 45000 each\n",
    "batches_atmosphere = split_into_batches(atmosphere_tasks, batch_size=45000)\n",
    "\n",
    "batch_file_names_atmosphere = save_batches_to_jsonl(batches_atmosphere, prefix=\"batch_tasks_atmosphere\")\n",
    "batch_file_ids_atmosphere = upload_batch_files(batch_file_names_atmosphere)\n",
    "batch_job_ids_atmosphere = start_batch_jobs(batch_file_ids_atmosphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_job_ids_food' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Collect all batch job IDs into one list\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m batch_job_ids_sentiment \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_job_ids_food\u001b[49m \u001b[38;5;241m+\u001b[39m batch_job_ids_service \u001b[38;5;241m+\u001b[39m batch_job_ids_atmosphere\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_job_ids_food' is not defined"
     ]
    }
   ],
   "source": [
    "# Collect all batch job IDs into one list\n",
    "batch_job_ids_sentiment = batch_job_ids_food + batch_job_ids_service + batch_job_ids_atmosphere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Category summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOOD_SUMMARY_PROMPT = (\n",
    "    \"You are an expert summarizer specializing in customer opinions about the food in a restaurant. \"\n",
    "    \"The summary should focus exclusively on customer perceptions of the food, including aspects like taste, presentation, freshness, and variety. \"\n",
    "    \"Do not include information about price, service, or atmosphere. \"\n",
    "    \"List up to the 5 most positively recommended items in a second section as bullet points. \"\n",
    "    \"Only include food items in the recommendations section if customers mention them positively. \"\n",
    "    \"Do not list items with mixed or negative reviews. \"\n",
    "    \"Write concisely, strictly in English and limit the overall response to around 400 characters.\\n\\n\"\n",
    "    \"Reviews:\\n{reviews}\\n\\n\"\n",
    "    \"Summary:\"\n",
    ")\n",
    "\n",
    "FOOD_COMBINE_PROMPT = (\n",
    "    \"You are an expert summarizer specializing in customer opinions about food. Combine the following two summaries into a single cohesive summary in English. \"\n",
    "    \"The combined summary should focus exclusively on customer perceptions of the food, including aspects like taste, presentation, freshness, and variety. \"\n",
    "    \"Do not include information about price, service, or atmosphere. \"\n",
    "    \"List up to the 5 most positively recommended items in a second section as bullet points. \"\n",
    "    \"Only include food items in the recommendations section if customers mention them positively. \"\n",
    "    \"Do not list items with mixed or negative reviews. \"\n",
    "    \"Limit the overall response to around 400 characters:\\n\\n\"\n",
    "    \"Summary 1:\\n{summary1}\\n\\n\"\n",
    "    \"Summary 2:\\n{summary2}\\n\\n\"\n",
    "    \"Combined Summary:\"\n",
    ")\n",
    "\n",
    "SERVICE_SUMMARY_PROMPT = (\n",
    "    \"You are an expert summarizer specializing in customer opinions about the service in restaurants. \"\n",
    "    \"The summary should focus exclusively on customer perceptions of the service, including aspects like speed, attentiveness, friendliness, and professionalism. \"\n",
    "    \"Do not include information about price, food, or atmosphere. \"\n",
    "    \"Write concisely, strictly in English and limit the overall response to around 400 characters.\\n\\n\"\n",
    "    \"Reviews:\\n{reviews}\\n\\n\"\n",
    "    \"Summary:\"\n",
    ")\n",
    "\n",
    "SERVICE_COMBINE_PROMPT = (\n",
    "    \"You are an expert summarizer specializing in customer opinions about service in restaurants. Combine the following two summaries into a single cohesive summary in English.\"\n",
    "    \"The combined summary should focus exclusively on customer perceptions of the service, including aspects like speed, attentiveness, friendliness, and professionalism. \"\n",
    "    \"Do not include information about price, food, or atmosphere. \"\n",
    "    \"Limit the overall response to around 400 characters:\\n\\n\"\n",
    "    \"Summary 1:\\n{summary1}\\n\\n\"\n",
    "    \"Summary 2:\\n{summary2}\\n\\n\"\n",
    "    \"Combined Summary:\"\n",
    ")\n",
    "\n",
    "ATMOSPHERE_SUMMARY_PROMPT = (\n",
    "    \"You are an expert summarizer specializing in customer opinions about the atmosphere in restaurants. \"\n",
    "    \"The summary should focus exclusively on customer perceptions of the atmosphere, including aspects like ambiance, decor, cleanliness, noise levels, and overall vibe. \"\n",
    "    \"Do not include information about price, food, or service. \"\n",
    "    \"Write concisely, strictly in English and limit the overall response to around 400 characters.\\n\\n\"\n",
    "    \"Reviews:\\n{reviews}\\n\\n\"\n",
    "    \"Summary:\"\n",
    ")\n",
    "\n",
    "ATMOSPHERE_COMBINE_PROMPT = (\n",
    "    \"You are an expert summarizer specializing in customer opinions about the atmosphere in restaurants. Combine the following two summaries into a single cohesive summary in English. \"\n",
    "    \"The combined summary should focus exclusively on customer perceptions of the atmosphere, including aspects like ambiance, decor, cleanliness, noise levels, and overall vibe. \"\n",
    "    \"Do not include information about price, food, or service. \"\n",
    "    \"Limit the overall response to around 400 characters:\\n\\n\"\n",
    "    \"Summary 1:\\n{summary1}\\n\\n\"\n",
    "    \"Summary 2:\\n{summary2}\\n\\n\"\n",
    "    \"Combined Summary:\"\n",
    ")\n",
    "\n",
    "PRICE_SUMMARY_PROMPT = (\n",
    "    \"You are an expert summarizer specializing in customer opinions about pricing in restaurants. \"\n",
    "    \"The summary should focus exclusively on customer perceptions of the price, including aspects like value for money, affordability, and pricing fairness. \"\n",
    "    \"Do not include information about food, service, or atmosphere. \"\n",
    "    \"Write concisely, strictly in English and limit the overall response to around 400 characters.\\n\\n\"\n",
    "    \"Reviews:\\n{reviews}\\n\\n\"\n",
    "    \"Summary:\"\n",
    ")\n",
    "\n",
    "PRICE_COMBINE_PROMPT = (\n",
    "    \"You are an expert summarizer specializing in customer opinions about pricing in restaurants. Combine the following two summaries into a single cohesive summary in English. \"\n",
    "    \"The combined summary should focus exclusively on customer perceptions of the price, including aspects like value for money, affordability, and pricing fairness. \"\n",
    "    \"Do not include information about food, service, or atmosphere. \"\n",
    "    \"Limit the overall response to around 400 characters:\\n\\n\"\n",
    "    \"Summary 1:\\n{summary1}\\n\\n\"\n",
    "    \"Summary 2:\\n{summary2}\\n\\n\"\n",
    "    \"Combined Summary:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the categorized sentences with reviews general to get the restaurant_ids\n",
    "reviews_df = pd.merge(category_df, data, on='review_id', how='left')\n",
    "# keep only the necessary columns\n",
    "reviews_df = reviews_df[['review_id', 'restaurant_id', 'food_sentences', 'service_sentences', 'atmosphere_sentences', 'price_sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 restaurant_id  \\\n",
      "0  ChIJ_VWb4xn6mUcRH4NujtHMKJI   \n",
      "1  ChIJo5EYOK_4mUcRi4shjNiEDUc   \n",
      "2  ChIJ_VfiMxj6mUcRRK_QBdxww7g   \n",
      "\n",
      "                                        summary_food  \\\n",
      "0  Customers have mixed feelings about the food. ...   \n",
      "1                                               None   \n",
      "2  Customers overwhelmingly praise the food, high...   \n",
      "\n",
      "                                     summary_service  \\\n",
      "0  Customer perceptions of service vary widely. M...   \n",
      "1  Customers consistently highlight the exception...   \n",
      "2  Customers appreciate the attentive service, no...   \n",
      "\n",
      "                                  summary_atmosphere  \\\n",
      "0  Customers appreciate the atmosphere of the res...   \n",
      "1  Customers appreciate the vibrant and lively at...   \n",
      "2  Customers appreciate the restaurant's beautifu...   \n",
      "\n",
      "                                       summary_price  user_count_food  \\\n",
      "0  Customer opinions on restaurant pricing vary, ...               49   \n",
      "1  Customers perceive the restaurant's pricing as...                0   \n",
      "2                                               None                2   \n",
      "\n",
      "   user_count_service  user_count_atmosphere  user_count_price  \n",
      "0                  39                     33                11  \n",
      "1                   8                     12                 5  \n",
      "2                   2                      1                 0  \n"
     ]
    }
   ],
   "source": [
    "# Extract unique restaurant IDs of the two batches\n",
    "unique_restaurant_ids = reviews_df['restaurant_id'].dropna().unique()\n",
    "\n",
    "summaries_categories = []  # List to store summaries\n",
    "\n",
    "# Generate summaries for each restaurant\n",
    "for restaurant_id in unique_restaurant_ids:\n",
    "\n",
    "    # Summarize food reviews\n",
    "    food_summary, user_count_food = summarize_reviews(restaurant_id, reviews_df, 'food_sentences', FOOD_SUMMARY_PROMPT, FOOD_COMBINE_PROMPT)\n",
    "    \n",
    "    # Summarize service reviews\n",
    "    service_summary, user_count_service = summarize_reviews(restaurant_id, reviews_df, 'service_sentences', SERVICE_SUMMARY_PROMPT, SERVICE_COMBINE_PROMPT)\n",
    "    \n",
    "    # Summarize atmosphere reviews\n",
    "    atmosphere_summary, user_count_atmosphere = summarize_reviews(restaurant_id, reviews_df, 'atmosphere_sentences', ATMOSPHERE_SUMMARY_PROMPT, ATMOSPHERE_COMBINE_PROMPT)\n",
    "    \n",
    "    # Summarize price reviews\n",
    "    price_summary, user_count_price = summarize_reviews(restaurant_id, reviews_df, 'price_sentences', PRICE_SUMMARY_PROMPT, PRICE_COMBINE_PROMPT)\n",
    "    \n",
    "    # Append the summaries to the list\n",
    "    summaries_categories.append({\n",
    "        \"restaurant_id\": restaurant_id,\n",
    "        \"summary_food\": food_summary,\n",
    "        \"summary_service\": service_summary,\n",
    "        \"summary_atmosphere\": atmosphere_summary,\n",
    "        \"summary_price\": price_summary,\n",
    "        \"user_count_food\": user_count_food,\n",
    "        \"user_count_service\": user_count_service,\n",
    "        \"user_count_atmosphere\": user_count_atmosphere,\n",
    "        \"user_count_price\": user_count_price,\n",
    "    })\n",
    "\n",
    "# Convert the list of summaries into a DataFrame\n",
    "summaries_categories_df = pd.DataFrame(summaries_categories)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(summaries_categories_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Transition - wait unitl 5. is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for all batch jobs to complete\n",
    "wait_for_batches_to_complete(client, batch_job_ids_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Retrieve subratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_batch_results_subratings(batch_job_ids, category):\n",
    "    results = []\n",
    "\n",
    "    for batch_job_id in batch_job_ids:\n",
    "        # Get batch job status\n",
    "        batch_job_status = client.batches.retrieve(batch_job_id)\n",
    "        result_file_id = batch_job_status.output_file_id\n",
    "\n",
    "        # ! think about error handeling !\n",
    "        if not result_file_id:\n",
    "            print(f\"No output file found for batch job {batch_job_id}\")\n",
    "            continue  # Skip if there's no result file\n",
    "\n",
    "        # Download the batch results file\n",
    "        result_content = client.files.content(result_file_id).content\n",
    "        result_file_name = f\"batch_results_{category}.jsonl\"\n",
    "\n",
    "        with open(result_file_name, 'wb') as file:\n",
    "            file.write(result_content)\n",
    "\n",
    "        # Read and parse the results\n",
    "        with open(result_file_name, 'r') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    response = json.loads(line.strip())\n",
    "                    custom_id = response['custom_id']  # Format: review_id_category\n",
    "                    review_id, _ = custom_id.rsplit(\"_\", 1)  # Extract review_id\n",
    "                    \n",
    "                    # Extract rating from response\n",
    "                    body = response.get(\"response\", {}).get(\"body\", {})\n",
    "                    choices = body.get(\"choices\", [])\n",
    "                    if choices:\n",
    "                        content = json.loads(choices[0][\"message\"][\"content\"])\n",
    "                        rating = content.get('rating', None)\n",
    "                    else:\n",
    "                        rating = None\n",
    "\n",
    "                    # Append results\n",
    "                    results.append({\"review_id\": int(review_id), f\"rating_{category}\": rating})\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing result for {category}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve results for each category\n",
    "df_food = retrieve_batch_results(batch_job_ids_food, \"food\")\n",
    "df_service = retrieve_batch_results(batch_job_ids_service, \"service\")\n",
    "df_atmosphere = retrieve_batch_results(batch_job_ids_atmosphere, \"atmosphere\")\n",
    "\n",
    "# Merge all results into a single DataFrame\n",
    "df_ratings = df_food.merge(df_service, on=\"review_id\", how=\"outer\").merge(df_atmosphere, on=\"review_id\", how=\"outer\")\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(df_ratings.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
