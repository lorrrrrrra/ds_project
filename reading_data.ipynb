{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "#import googlemaps\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API_basics = pd.read_csv(\"C:/Users/admin/Downloads/API_Basics.csv\")\n",
    "#API_general = pd.read_csv(\"C:/Users/admin/Downloads/API_General.csv\")\n",
    "#API_additional = pd.read_csv(\"C:/Users/admin/Downloads/API_Additional.csv\")\n",
    "\n",
    "API_basics = pd.read_csv(\"C:/Users/Theresa/Downloads/API_Basics.csv\")\n",
    "API_general = pd.read_csv(\"C:/Users/Theresa/Downloads/API_General.csv\")\n",
    "API_additional = pd.read_csv(\"C:/Users/Theresa/Downloads/API_Additional.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting information for the cities\n",
    "was already done, no need to change anda lso no need to run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API_basics[\"city\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key = \"AIzaSyCmU1xOszaGFTFecpdG8aEb7AGOGDOvB9c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to get the city boundaries\n",
    "# def get_city_boundaries(city_name):\n",
    "#     # Initialize the Google Maps client\n",
    "#     gmaps = googlemaps.Client(key=api_key) # googlemaps package\n",
    "    \n",
    "#     # Get the city boundaries\n",
    "#     geocode_result = gmaps.geocode(city_name)\n",
    "\n",
    "#     # save the boundaries\n",
    "#     low_lat = geocode_result[0]['geometry']['bounds']['southwest']['lat']\n",
    "#     low_long = geocode_result[0]['geometry']['bounds']['southwest']['lng']\n",
    "#     high_lat = geocode_result[0]['geometry']['bounds']['northeast']['lat']\n",
    "#     high_long = geocode_result[0]['geometry']['bounds']['northeast']['lng']\n",
    "\n",
    "    \n",
    "#     return low_lat, low_long, high_lat, high_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cities = pd.DataFrame(columns=['city_id', 'name', 'country', 'low_lat_value', 'low_long_value', 'high_lat_value', 'high_long_value', 'number_restaurants'])\n",
    "\n",
    "# for index, city in enumerate(API_basics[\"city\"].unique(), start=0):\n",
    "#     low_lat, low_long, high_lat, high_long = get_city_boundaries(city)\n",
    "    \n",
    "#     city_df = pd.DataFrame({\n",
    "#         'city_id': [index],\n",
    "#         'name': [city],\n",
    "#         'country': 'Germany',  # Assuming country is Germany\n",
    "#         'low_lat_value': [low_lat],\n",
    "#         'low_long_value': [low_long],\n",
    "#         'high_lat_value': [high_lat],\n",
    "#         'high_long_value': [high_long],\n",
    "#         'number_restaurants': [None]\n",
    "#     })\n",
    "\n",
    "#     cities = pd.concat([cities, city_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cities['name'] = cities['name'].str.replace('ü', 'ue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in cities.iterrows():\n",
    "#     city_id = row['city_id']\n",
    "#     filtered_df = API_basics[API_basics['city_id'] == city_id]\n",
    "#     unique_restaurants_count = filtered_df['id'].nunique()\n",
    "#     cities.at[index, 'number_restaurants'] = unique_restaurants_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cities.to_csv('cities.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing API Tables for input into the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Basics\n",
    "- getting lat and long data works \n",
    "- we don't need the primarytypeDisplayName as we have the primaryType\n",
    "- I have not yet figured out, how to get the name from the json... :( maybe we can cut it after text and before languageCode? I think that will be the easiest solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current structure of the table in the database\n",
    "# CREATE TABLE Restaurant_Basics (\n",
    "#     restaurant_id VARCHAR PRIMARY KEY,\n",
    "#     city_id INT REFERENCES Cities(city_id),\n",
    "#     name VARCHAR,\n",
    "#     primary_type VARCHAR,\n",
    "#     types VARCHAR,\n",
    "#     business_status BOOLEAN,\n",
    "#     pure_service_area BOOLEAN,\n",
    "#     address VARCHAR,\n",
    "#     lat_value FLOAT,\n",
    "#     long_value FLOAT\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the city name into an id\n",
    "cities = pd.read_csv('cities.csv')\n",
    "city_dict = pd.Series(cities.city_id.values, index=cities.name).to_dict()\n",
    "\n",
    "API_basics['city'] = API_basics['city'].str.replace('ü', 'ue')\n",
    "API_basics['city_id'] = API_basics['city'].map(city_dict)\n",
    "API_basics.drop(\"city\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting lat and long data\n",
    "for index, row in API_basics.iterrows():\n",
    "    location_value = json.loads(row['location'].replace(\"'\", '\"'))\n",
    "    API_basics.at[index, 'lat_value'] = location_value['latitude']\n",
    "    API_basics.at[index, 'long_value'] = location_value['longitude']\n",
    "\n",
    "API_basics.drop(\"location\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the primaryTypeDisplayName\n",
    "API_basics.drop(\"primaryTypeDisplayName\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the column name to better understand that this is not the \"finished\" name but rather the json file with the name in it\n",
    "API_basics.rename(columns = {'name': 'name_json'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # getting name and language code for name \n",
    "# for index, row in API_basics.iterrows():\n",
    "#     if row['name_json'][9] == \"'\":\n",
    "#         try:\n",
    "#             json_string = row['name_json'].replace(\"'\", '\"')\n",
    "#             json_string = json_string.replace(r'\\\"', '\"')  \n",
    "#             json_string = json_string.replace(\"'text'\", '\"text\"').replace(\"'languageCode'\", '\"languageCode\"').replace(\"'de'\", '\"de\"').replace(\"'en'\", '\"en')\n",
    "        \n",
    "#             parsed_data = json.loads(json_string)\n",
    "            \n",
    "#             # Extrahiere 'languageCode' und 'text' aus den JSON-Daten\n",
    "#             API_basics.at[index, 'language_code_name'] = parsed_data['languageCode']\n",
    "#             API_basics.at[index, 'name'] = parsed_data['text']\n",
    "    \n",
    "#         except json.JSONDecodeError:\n",
    "#             print(f\"Error decoding JSON for index {index}: {row['name_json']}\")\n",
    "\n",
    "    \n",
    "#     elif row['name_json'][9] == '\"':\n",
    "#         try:\n",
    "#             json_string = row['name_json'].replace(\"'text'\", '\"text\"').replace(\"'languageCode'\", '\"languageCode\"').replace(\"'de'\", '\"de\"').replace(\"'en'\", '\"en')\n",
    "#             parsed_data = json.loads(json_string)\n",
    "\n",
    "#             # Extrahiere 'languageCode' und 'text' aus den JSON-Daten\n",
    "#             API_basics.at[index, 'language_code_name'] = parsed_data['languageCode']\n",
    "#             API_basics.at[index, 'name'] = parsed_data['text']\n",
    "    \n",
    "\n",
    "#         except json.JSONDecodeError:\n",
    "#             print(f\"b - Error decoding JSON for index {index}: {row['name']}\")\n",
    "#     ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def clean_and_parse_json(json_string):\n",
    "#     \"\"\"\n",
    "#     Cleans and parses a JSON string, handling common formatting issues.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Replace single quotes with double quotes for JSON compatibility\n",
    "#         json_string = json_string.replace(\"'\", '\"')\n",
    "#         # Remove escaped quotes for better parsing\n",
    "#         json_string = json_string.replace(r'\\\"', '\"')\n",
    "#         # Try parsing the cleaned JSON\n",
    "#         parsed_data = json.loads(json_string)\n",
    "#         return parsed_data\n",
    "#     except json.JSONDecodeError:\n",
    "#         return None  # Return None if parsing fails\n",
    "\n",
    "# # Process each row\n",
    "# for index, row in API_basics.iterrows():\n",
    "#     json_string = row.get('name_json', '')\n",
    "#     if not isinstance(json_string, str):\n",
    "#         print(f\"Skipping index {index}: 'name_json' is not a string\")\n",
    "#         continue\n",
    "\n",
    "#     # Attempt to clean and parse the JSON string\n",
    "#     parsed_data = clean_and_parse_json(json_string)\n",
    "#     if parsed_data:\n",
    "#         # Ensure keys exist before assigning\n",
    "#         language_code = parsed_data.get('languageCode')\n",
    "#         name = parsed_data.get('text')\n",
    "\n",
    "#         if language_code and name:\n",
    "#             API_basics.at[index, 'language_code_name'] = language_code\n",
    "#             API_basics.at[index, 'name'] = name\n",
    "#         else:\n",
    "#             print(f\"Index {index}: Missing 'languageCode' or 'text' in parsed JSON\")\n",
    "#     else:\n",
    "#         print(f\"Index {index}: Error decoding JSON: {json_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply json.dumps to each element in the 'name_transformed' column\n",
    "API_basics['name_string'] = API_basics['name_json'].apply(\n",
    "    lambda x: json.dumps(x, ensure_ascii=False) if isinstance(x, dict) else str(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 'text' and 'languageCode' in separate columns\n",
    "API_basics['API_name'] = API_basics['name_string'].str.extract(r\"'text': ?['\\\"](.*)['\\\"].*'languageCode':\")\n",
    "API_basics['language_code'] = API_basics['name_string'].str.extract(r\"'languageCode': ['\\\"]([^'\\\"]*)['\\\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"text\": \"s\"Casa\", \"languageCode\": \"de\"}'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "API_basics['name'][7].replace(\"'\", '\"').replace('\"{', '{').replace('}\"', '}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': \"Bakery Schmid 'cafe cult'\", 'languageCode': 'en'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = API_basics['name'][10].replace(\"'text'\", '\"text\"').replace(\"'languageCode'\", '\"languageCode\"').replace(\"'de'\", '\"de\"').replace(\"'en'\", '\"en\"')\n",
    "print(string)\n",
    "# print(re.sub(r\"(?<=:)([^,}]+)(?=\\s*[},])\", lambda match: match.group(0).replace(\"'\", \"\\\"\"), string))\n",
    "\n",
    "# re.sub(r\"(?<!\\\\)'\", '\"', json_string)\n",
    "# json.loads(string)\n",
    "\n",
    "json.loads('{\"text\": \"Bakery Schmid \\'cafe cult\\'\", \"languageCode\": \"en\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nan'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "API_basics['name'][10].replace(\"'text'\", '\"text\"').replace(\"'languageCode'\", '\"languageCode\"').replace(\"'de'\", '\"de\"').replace(\"'en'\", '\"en\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mAPI_basics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "API_basics['name'][10][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_language(input_string):\n",
    "    # Regulärer Ausdruck, um sowohl einfache als auch doppelte Anführungszeichen zu berücksichtigen\n",
    "    text_match = re.search(r\"(?:'text'|\\\"text\\\").*?[\\'\\\"]([^\\'\\\"]*)[\\'\\\"]\", input_string)\n",
    "    language_code_match = re.search(r\"(?:'languageCode'|\\\"languageCode\\\").*?[\\'\\\"]([^\\'\\\"]*)[\\'\\\"]\", input_string)\n",
    "    \n",
    "    if text_match and language_code_match:\n",
    "        # Extrahierter Text\n",
    "        text = text_match.group(1)\n",
    "        # Extrahierter languageCode\n",
    "        language_code = language_code_match.group(1)\n",
    "        return text, language_code\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_text_and_language(API_basics['name'][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicates (important as there are some!)\n",
    "duplicates = API_basics[API_basics.duplicated(subset=['name'], keep=False)]\n",
    "API_basics = API_basics.drop_duplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['primary_type_display'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# renaming columns to match the database, database columns are named in snake_case NOT camelCase \u001b[39;00m\n\u001b[0;32m      2\u001b[0m API_basics \u001b[38;5;241m=\u001b[39m API_basics\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrestaurant_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimaryType\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimaryTypeDisplayName\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary_type_display\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      3\u001b[0m                                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbusinessStatus\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbusiness_status\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpureServiceAreaBusiness\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpure_service_area\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m                                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformattedAddress\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m----> 5\u001b[0m API_basics \u001b[38;5;241m=\u001b[39m \u001b[43mAPI_basics\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrestaurant_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcity_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprimary_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprimary_type_display\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtypes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbusiness_status\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpure_service_area\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maddress\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlat_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlong_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Theresa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Theresa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6116\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6118\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Theresa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6178\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6177\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6178\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['primary_type_display'] not in index\""
     ]
    }
   ],
   "source": [
    "# renaming columns to match the database, database columns are named in snake_case NOT camelCase \n",
    "API_basics = API_basics.rename(columns={'id': 'restaurant_id', 'primaryType': 'primary_type', 'primaryTypeDisplayName': 'primary_type_display', \n",
    "                                        'businessStatus': 'business_status', 'pureServiceAreaBusiness': 'pure_service_area',\n",
    "                                        'formattedAddress': 'address'})\n",
    "API_basics = API_basics[['restaurant_id', 'city_id', 'name', 'primary_type', 'primary_type_display', 'types', 'business_status', \n",
    "                        'pure_service_area', 'address', 'lat_value', 'long_value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_basics.to_csv('API_basics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API general\n",
    "the only thing that is missing here are the opening hours, every other column is already processed and uploaded at the database (it would the best to save the opening hours as a json as we can then just input them in the table without changing its structure)\n",
    "\n",
    "\n",
    "-> my problem was that I didn't know which day corresponds to which as there are only 6 in the json..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract string from json\n",
    "API_general['opening_hours'] = API_general['regularOpeningHours'].apply(json.dumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression to extract the opening and closing times\n",
    "pattern = re.compile(r\"'open':\\s*\\{'day':\\s*(\\d),\\s*'hour':\\s*(\\d+),\\s*'minute':\\s*(\\d+)\\},\\s*'close':\\s*\\{'day':\\s*\\d,\\s*'hour':\\s*(\\d+),\\s*'minute':\\s*(\\d+)\\}\")\n",
    "\n",
    "# Days of the week for reference (0 = Monday, 1 = Tuesday, ..., 6 = Sunday)\n",
    "days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Iterate over each entry in the Series and extract opening hours\n",
    "opening_hours_json = []\n",
    "\n",
    "for entry in API_general['opening_hours']:\n",
    "    matches = pattern.findall(entry)\n",
    "\n",
    "    # Initialize a dictionary to hold the opening hours for each weekday\n",
    "    weekly_hours = {}\n",
    "\n",
    "    for match in matches:\n",
    "        # Convert the minute values to integers and format them correctly\n",
    "        open_time = f\"{int(match[1])}:{int(match[2]):02d}\"\n",
    "        close_time = f\"{int(match[3])}:{int(match[4]):02d}\"\n",
    "        day_name = days_of_week[int(match[0])]  # Convert open_day to day name\n",
    "\n",
    "        # Store the opening hours for each day\n",
    "        weekly_hours[day_name] = {'open': open_time, 'close': close_time}\n",
    "    \n",
    "    # Add the result to the list\n",
    "    opening_hours_json.append(json.dumps(weekly_hours))  # Store as JSON\n",
    "\n",
    "API_general['opening_hours_json'] = opening_hours_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current structure of the table in the database\n",
    "# CREATE TABLE restaurant_general (\n",
    "#     restaurant_id VARCHAR PRIMARY KEY,\n",
    "#     containing_places BOOLEAN,\n",
    "#     phone_number VARCHAR,\n",
    "#     website_uri VARCHAR,\n",
    "#     summary JSON,\n",
    "#     opening_hours JSON,\n",
    "#     price_level VARCHAR,\n",
    "#     price_range JSON,\n",
    "#     google_rating FLOAT,\n",
    "#     google_user_rating_count FLOAT\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API_general['regularOpeningHours'][1]\n",
    "# result = re.sub(r\"'weekdayDescriptions':.*\", \"'weekdayDescriptions': []}\", API_general['regularOpeningHours'][0])\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in API_general.iterrows():\n",
    "#     if row\n",
    "#     string = re.sub(r\"'weekdayDescriptions':.*\", \"'weekdayDescriptions': []}\", row['regularOpeningHours'])\n",
    "#     json_string = string.replace(\"'\", '\"').replace(\"True\", \"true\")\n",
    "#     opening_hours = json.loads(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API_general['regularOpeningHours'][1] == np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'startPrice': {'currencyCode': 'EUR', 'units': '1'},\n",
       " 'endPrice': {'currencyCode': 'EUR', 'units': '10'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(API_general['priceRange'][0].replace(\"'\", '\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_to_json(string):\n",
    "    if pd.notna(string):\n",
    "        try: \n",
    "            json_value = json.loads(string.replace(\"'\", '\"'))\n",
    "            return json_value\n",
    "        except Exception as e:\n",
    "            print(\"Fehler:\", e)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Laid-back hotel featuring a bar, a restaurant & a terrace, plus meeting & event space.',\n",
       " 'languageCode': 'en'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_to_json(API_general['editorialSummary'][23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehler: Expecting ',' delimiter: line 1 column 98 (char 97)\n",
      "Fehler: Expecting ',' delimiter: line 1 column 97 (char 96)\n",
      "Fehler: Expecting ',' delimiter: line 1 column 101 (char 100)\n",
      "Fehler: Expecting ',' delimiter: line 1 column 69 (char 68)\n",
      "Fehler: Expecting ',' delimiter: line 1 column 81 (char 80)\n",
      "Fehler: Expecting ',' delimiter: line 1 column 97 (char 96)\n",
      "Fehler: Expecting ',' delimiter: line 1 column 100 (char 99)\n",
      "Fehler: Expecting ',' delimiter: line 1 column 44 (char 43)\n",
      "Fehler: Expecting ',' delimiter: line 1 column 98 (char 97)\n",
      "Fehler: Expecting ',' delimiter: line 1 column 98 (char 97)\n",
      "Fehler: Expecting ',' delimiter: line 1 column 17 (char 16)\n"
     ]
    }
   ],
   "source": [
    "API_general[\"price_range\"] = API_general[\"priceRange\"].apply(make_to_json)\n",
    "API_general[\"editorial_summary\"] = API_general[\"editorialSummary\"].apply(make_to_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(API_general[\"price_range\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['restaurant_id', 'containing_places', 'phone_number', 'website_uri',\n",
       "       'price_level', 'price_range', 'google_rating',\n",
       "       'google_user_rating_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "API_general.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns to match the database\n",
    "API_general = API_general.rename(columns={'id': 'restaurant_id', 'containingPlaces': 'containing_places', 'internationalPhoneNumber': 'phone_number', \n",
    "                                        'placesWebsiteUri': 'website_uri', 'priceLevel': 'price_level', \n",
    "                                        'userRatingCount': 'google_user_rating_count', 'rating': 'google_rating'})\n",
    "API_general = API_general[['restaurant_id', 'containing_places', 'phone_number', 'website_uri', 'price_level', 'price_range', 'google_rating', \n",
    "                        'google_user_rating_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_general.to_csv('API_general.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API additional\n",
    "should be all done and is already uploaded to the database on the server, but you can have a look at it whether it is correct or I made a mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_additional['accessibilityOptions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all of the keys in parking Options:\n",
    "all_keys = set()\n",
    "\n",
    "for entry in API_additional['parkingOptions']:\n",
    "    if not pd.isna(entry):  # Wenn der Eintrag nicht None oder leer ist\n",
    "        try:\n",
    "            # Lade den JSON-String in ein Dictionary\n",
    "            json_string = entry.replace(\"'\", '\"').replace(\"True\", \"true\").replace(\"False\", \"false\")\n",
    "            parking_dict = json.loads(json_string)\n",
    "            \n",
    "            # Füge die Schlüssel des Dictionaries zum Set hinzu\n",
    "            all_keys.update(parking_dict.keys())\n",
    "        except (json.JSONDecodeError, TypeError) as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "print(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parking_info(parking_options_string):\n",
    "    try:\n",
    "        if not pd.isna(parking_options_string):\n",
    "            json_string = parking_options_string.replace(\"'\", '\"').replace(\"True\", \"true\").replace(\"False\", \"false\")\n",
    "            parking_dict = json.loads(json_string)\n",
    "    \n",
    "            # Extrahiere relevante Informationen, Standardwert False falls der Schlüssel nicht vorhanden ist\n",
    "            free_parking_lot = parking_dict.get('freeParkingLot', np.nan)\n",
    "            paid_parking_lot = parking_dict.get('paidParkingLot', np.nan)\n",
    "            free_street_parking = parking_dict.get('freeStreetParking', np.nan)\n",
    "            paid_street_parking = parking_dict.get('paidStreetParking', np.nan)\n",
    "            free_garage_parking = parking_dict.get('freeGarageParking', np.nan)\n",
    "            paid_garage_parking = parking_dict.get('paidGarageParking', np.nan)\n",
    "            valet_parking = parking_dict.get('valetParking', np.nan)\n",
    "\n",
    "        else:\n",
    "            free_parking_lot = np.nan\n",
    "            paid_parking_lot = np.nan\n",
    "            free_street_parking = np.nan\n",
    "            paid_street_parking = np.nan\n",
    "            free_garage_parking = np.nan\n",
    "            paid_garage_parking = np.nan\n",
    "            valet_parking = np.nan\n",
    "            \n",
    "    \n",
    "    except (json.JSONDecodeError, TypeError) as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return (np.nan, np.nan, np.nan, np.nan, np.nan, np.nan)   \n",
    "    \n",
    "    return (free_parking_lot, paid_parking_lot, free_street_parking, paid_street_parking, free_garage_parking, paid_garage_parking, valet_parking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying method to get all options\n",
    "API_additional[['free_parking_lot', 'paid_parking_lot', 'free_street_parking', 'paid_street_parking', 'free_garage_parking', 'paid_garage_parking', 'valet_parking']] = API_additional['parkingOptions'].apply(extract_parking_info).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all of the keys in parking Options:\n",
    "all_keys = set()\n",
    "\n",
    "for entry in API_additional['paymentOptions']:\n",
    "    if not pd.isna(entry):  \n",
    "        try:\n",
    "            # loading the json-string to a dict\n",
    "            json_string = entry.replace(\"'\", '\"').replace(\"True\", \"true\").replace(\"False\", \"false\")\n",
    "            parking_dict = json.loads(json_string)\n",
    "            \n",
    "            # adding the keys of the dict to the set\n",
    "            all_keys.update(parking_dict.keys())\n",
    "        except (json.JSONDecodeError, TypeError) as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "print(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_payment_info(payment_options_string):\n",
    "    try:\n",
    "        if not pd.isna(payment_options_string):\n",
    "            json_string = payment_options_string.replace(\"'\", '\"').replace(\"True\", \"true\").replace(\"False\", \"false\")\n",
    "            parking_dict = json.loads(json_string)\n",
    "    \n",
    "            # extract relevant information, nan if key is not there\n",
    "            accepts_debit_cards = parking_dict.get('acceptsDebitCards', np.nan)\n",
    "            accepts_credit_cards = parking_dict.get('acceptsCreditCards', np.nan)\n",
    "            accepts_cash_only = parking_dict.get('acceptsCashOnly', np.nan)\n",
    "            accepts_nfc = parking_dict.get('acceptsNfc', np.nan)\n",
    "\n",
    "        else:\n",
    "            accepts_debit_cards = np.nan\n",
    "            accepts_credit_cards = np.nan\n",
    "            accepts_cash_only = np.nan\n",
    "            accepts_nfc = np.nan\n",
    "            \n",
    "    \n",
    "    except (json.JSONDecodeError, TypeError) as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return (np.nan, np.nan, np.nan, np.nan)   \n",
    "    \n",
    "    return (accepts_debit_cards, accepts_credit_cards, accepts_cash_only, accepts_nfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying method to get all options\n",
    "API_additional[['accepts_debit_cards', 'accepts_credit_cards', 'accepts_cash_only', 'accepts_nfc']] = API_additional['paymentOptions'].apply(extract_payment_info).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all of the keys in accessibility Options:\n",
    "all_keys = set()\n",
    "\n",
    "for entry in API_additional['accessibilityOptions']:\n",
    "    if not pd.isna(entry):  # Wenn der Eintrag nicht None oder leer ist\n",
    "        try:\n",
    "            # loading the json-string to a dict\n",
    "            json_string = entry.replace(\"'\", '\"').replace(\"True\", \"true\").replace(\"False\", \"false\")\n",
    "            parking_dict = json.loads(json_string)\n",
    "            \n",
    "            # adding the keys of the dict to the set\n",
    "            all_keys.update(parking_dict.keys())\n",
    "        except (json.JSONDecodeError, TypeError) as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "print(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_accessibility_info(accessibility_options_string):\n",
    "    try:\n",
    "        if not pd.isna(accessibility_options_string):\n",
    "            json_string = accessibility_options_string.replace(\"'\", '\"').replace(\"True\", \"true\").replace(\"False\", \"false\")\n",
    "            parking_dict = json.loads(json_string)\n",
    "    \n",
    "            # extract relevant information, nan if key is not there\n",
    "            wheelchair_accessible_restroom = parking_dict.get('wheelchairAccessibleRestroom', np.nan)\n",
    "            wheelchair_accessible_entrance = parking_dict.get('wheelchairAccessibleEntrance', np.nan)\n",
    "            wheelchair_accessible_parking = parking_dict.get('wheelchairAccessibleParking', np.nan)\n",
    "            wheelchair_accessible_seating = parking_dict.get('wheelchairAccessibleSeating', np.nan)\n",
    "\n",
    "        else:\n",
    "            wheelchair_accessible_restroom = np.nan\n",
    "            wheelchair_accessible_entrance = np.nan\n",
    "            wheelchair_accessible_parking = np.nan\n",
    "            wheelchair_accessible_seating = np.nan\n",
    "            \n",
    "    \n",
    "    except (json.JSONDecodeError, TypeError) as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return (np.nan, np.nan, np.nan, np.nan)   \n",
    "    \n",
    "    return (wheelchair_accessible_restroom, wheelchair_accessible_entrance, wheelchair_accessible_parking, wheelchair_accessible_seating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying method to get all options\n",
    "API_additional[['wheelchair_accessible_restroom', 'wheelchair_accessible_entrance', 'wheelchair_accessible_parking', 'wheelchair_accessible_seating']] = API_additional['accessibilityOptions'].apply(extract_accessibility_info).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_additional.drop(\"parkingOptions\", axis=1, inplace=True)\n",
    "API_additional.drop(\"paymentOptions\", axis=1, inplace=True)\n",
    "API_additional.drop(\"accessibilityOptions\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_additional.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns to match the database\n",
    "API_additional = API_additional.rename(columns={'id': 'restaurant_id', 'curbsidePickup': 'curbside_pickup', 'dineIn': 'dine_in', \n",
    "                                        'liveMusic': 'live_music', 'outdoorSeating': 'outdoor_seating', 'servesBeer': 'serves_beer', \n",
    "                                        'servesBreakfast': 'serves_breakfast', 'servesBrunch': 'serves_brunch', \n",
    "                                        'servesCocktails': 'serves_cocktails', 'servesCoffee': 'serves_coffee', 'servesDessert': 'serves_dessert', \n",
    "                                        'servesDinner': 'serves_dinner', 'servesLunch': 'serves_lunch',\n",
    "                                        'servesVegetarianFood': 'serves_vegetarian_food', 'servesWine': 'serves_wine', 'allowsDogs': 'allows_dogs',\n",
    "                                        'goodForChildren': 'good_for_children', 'goodForGroups': 'good_for_groups',\n",
    "                                        'goodForWatchingSports': 'good_for_watching_sports', 'menuForChildren': 'menu_for_children'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting duplicates\n",
    "API_additional = API_additional.drop_duplicates(subset=['restaurant_id'])\n",
    "\n",
    "# changing np.nan to NULL\n",
    "API_additional = API_additional.where(pd.notna(API_additional), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_additional.to_csv('API_additional.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
