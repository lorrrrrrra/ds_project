{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load the CSV file\n",
    "batches_to_keys = pd.read_csv(\"batches_OpenAI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch_job_id  batch_6796154b62a481909fbda982a027349a: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id  batch_6796154e85708190a00827696e39cd50: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id  batch_67961eb62c5c8190ba132744e53eef87: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id   batch_67961eb8f0bc81909061266ddec5ff0f: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_6796270cd7c48190b524d5e3539a5437: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_6796270f8f9481909efa4fa8cc803c35: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_679627d54b808190bdca2838cf87585c: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_679627d7ee5481909f8ae5c1e07381fe: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_67962a9f9b2481908238f7ac692728a2: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_67962aa26edc81909e2d66a1728b5cff: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_67963ece4a1881909f894ae63ae51640: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_67963ed1778c8190b3707f33ba21b432: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_67963f51a708819088622e7d256eb29d: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_67963f54c9bc8190873c94d5a0720a86: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_6796403eba38819089144f8c5fd532b4: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_67964040cf3c8190aa94119f80f56fcb: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_679654d9f594819080a4de0bcc3b4371: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_679654dcb9d48190b706a0aedbd8a9de: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_6796555ae5788190a1f13b4fd711e046: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_6796555d185c8190a10f9a298ed1948d: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_679664a9c14081909953740ab30d62d2: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_679664acb91c819085478efc80a54ebd: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_679664b052dc8190a1a954aa3b75653f: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_679664b377208190a6e520210da5a082: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_679664b6960c8190babd755c82aef3f5: 'OpenAI' object has no attribute 'batch_jobs'\n",
      "Error processing batch_job_id batch_679664b9db408190a7ca2ad8172a0df3: 'OpenAI' object has no attribute 'batch_jobs'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing batch_job_id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_job_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Combine all results into a single DataFrame\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m final_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_categories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Save the final DataFrame to a CSV file\u001b[39;00m\n\u001b[0;32m     69\u001b[0m final_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextracted_category_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\_Juliana\\Uni_MA\\3.Semester\\DSP\\ds_project\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\_Juliana\\Uni_MA\\3.Semester\\DSP\\ds_project\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32mc:\\_Juliana\\Uni_MA\\3.Semester\\DSP\\ds_project\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# Iterate through each row in the CSV file\n",
    "all_categories = []  # To collect all category data\n",
    "\n",
    "for _, row in batches_to_keys.iterrows():\n",
    "    batch_job_id = row['batch_job_id']\n",
    "    api_key = row['api_key']\n",
    "    \n",
    "    # Create the OpenAI client\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    try:\n",
    "        # Get the results file ID for the batch job (assuming this comes from an API or prior step)\n",
    "        result_file_id = client.batch_jobs.retrieve(batch_job_id).output_file_id\n",
    "        \n",
    "        # Download the results file\n",
    "        result_content = client.files.content(result_file_id).content\n",
    "        result_file_name = \"batch_results.jsonl\"\n",
    "        with open(result_file_name, 'wb') as file:\n",
    "            file.write(result_content)  # Save the content to a file\n",
    "\n",
    "        # Parse the results\n",
    "        results = []\n",
    "        with open(result_file_name, 'r') as file:\n",
    "            for line in file:\n",
    "                results.append(json.loads(line.strip()))\n",
    "        results = pd.DataFrame(results)\n",
    "\n",
    "        # Extract the sentences for each topic\n",
    "        def extract_sentences(response):\n",
    "            try:\n",
    "                # Parse the assistant's message content\n",
    "                content = json.loads(response['body']['choices'][0]['message']['content'])\n",
    "                return {\n",
    "                    \"food_sentences\": \" \".join(content.get(\"food_sentences\", [])),\n",
    "                    \"service_sentences\": \" \".join(content.get(\"service_sentences\", [])),\n",
    "                    \"atmosphere_sentences\": \" \".join(content.get(\"atmosphere_sentences\", [])),\n",
    "                    \"price_sentences\": \" \".join(content.get(\"price_sentences\", [])),\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing response: {e}\")\n",
    "                return {\n",
    "                    \"food_sentences\": None,\n",
    "                    \"service_sentences\": None,\n",
    "                    \"atmosphere_sentences\": None,\n",
    "                    \"price_sentences\": None,\n",
    "                }\n",
    "\n",
    "        # Apply the extraction to the 'response' column\n",
    "        category_data = results['response'].apply(extract_sentences)\n",
    "\n",
    "        # Create a new DataFrame with the extracted category sentences\n",
    "        category_df = pd.DataFrame(category_data.tolist())\n",
    "\n",
    "        # Combine 'custom_id' and extracted category sentences\n",
    "        category_df['custom_id'] = results['custom_id']\n",
    "        # Rename custom_id to review_id\n",
    "        category_df.rename(columns={\"custom_id\": \"review_id\"}, inplace=True)\n",
    "\n",
    "        # Append the results for this batch to the collection\n",
    "        all_categories.append(category_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch_job_id {batch_job_id}: {e}\")\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "final_df = pd.concat(all_categories, ignore_index=True)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final_df.to_csv(\"extracted_category_data.csv\", index=False)\n",
    "print(\"Processing complete. Results saved to 'extracted_category_data.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
